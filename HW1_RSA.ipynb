{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18297b95be47e6ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:13:57.889461Z",
     "start_time": "2025-11-10T13:13:57.886996Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from cmdstanpy import CmdStanModel\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d513ff0b07056634",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Homework 1: Bayesian Cognitive and Rational Speech Act models\n",
    "\n",
    "This homework assignment is to be completed in groups. It is due on November 27, 2025 (midnight). Please upload *all files you created or modified* to the homework folder of your group in studIP.\n",
    "\n",
    "Group number:\n",
    "\n",
    "Names:\n",
    "\n",
    "*General note: It is permitted to use AI tools for coding. Please refer to the uploaded manual `AI_Tools_Guidelines` for recommended ways how to use AI to advance your studies in a way that supports your learning. That means that you should not be satisfied if an AI tool hands you a working version of your code, but that you should put in effort to understand how exactly the problem is solved. Another note of caution: What might work for large programming languages like Python, does not necessarily work for Stan. Check your code carefully and do NOT blindly trust AI.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85560d33a555fddf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Introduction\n",
    "During the past weeks, you have learned how Bayesian inference works and how it can be used in Bayesian cognitive models. You also learned about a specific type of Bayesian models that can be used to model pragmatic language understanding and production, the Rational Speech Act models. The goal of this homework assignment is for you to learn how to implement Bayesian models in Stan and, specifically, how to implement RSA models in Stan. A special focus will be on the different use cases and evaluation methods of RSA models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9653bd24afeb9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1) Stan modeling (8 points)\n",
    "\n",
    "1.1) In the file `simple_model.stan`, you will find a simple Stan model. Describe its implementation, relating it to the knowledge you gained about the conventions for coding models in Stan. (4 points)\n",
    "\n",
    "1.2) You will notice that the model does not compile. Fix the problems and explain what you did. (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833df871307041bb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2) Bayesian cognitive models (10 points)\n",
    "Think of a use case for `simple_model.stan` in the scope of Bayesian cognitive modeling. Describe the model while answering the following questions:\n",
    "\n",
    "2.1) What cognitive capacity can be explained by this model? (2 points) \n",
    "\n",
    "2.2) What is the purpose and function of this model? (3 points)\n",
    "\n",
    "2.3) At which level of analysis does it model this cognitive capacity and why? (3 points)\n",
    "\n",
    "Overall coherence gives another 2 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9000d37e600f7898",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3) RSA modeling (82 points)\n",
    "The purpose of the following model is to explain the use of overinformative referring expressions in pragmatic communication. In referential communication, the speaker’s task is to produce a referring expression that allows a listener to identify the target in the context. Consider the context below, where the target is the small blue pin. A referring expression including a size adjective (the small pin) is strictly speaking sufficient for uniquely establishing reference to the target, yet speakers often “overmodify” with color, producing referring expressions like the small blue pin. This overmodification phenomenon is what the model is intended to capture.\n",
    "\n",
    "<img src=\"img/size-sufficient.png\" width=\"400\"/>\n",
    "\n",
    "### 3.1) Vanilla RSA (20 points)\n",
    "In the file `vanilla_rsa.stan`, you find an RSA model of the production of referring expressions, based on the vanilla RSA model of Frank & Goodman (2012) that we discussed in class.\n",
    "\n",
    "3.1.1) Provide informative comments in the file `vanilla_rsa.stan`. (4 points)\n",
    "\n",
    "3.1.2) You will notice that the parameters and model blocks are empty. Why is that? Go through the following code and inspect the model's behavior. Look at the stan variables that are included in the fitted model. (3 points)\n",
    "\n",
    "    There are no parameters and no defined model, as we are not doing Bayesian sampling. We are not trying to estimate any values but rather arrive at our results deterministically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33f4060872b2af5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T09:58:12.006198Z",
     "start_time": "2025-11-11T09:58:11.938342Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:43:18 - cmdstanpy - INFO - compiling stan file C:\\Users\\User\\Proton Drive\\maa_ng0h\\My files\\C. Work\\2E Modeling Communication and Abstraction\\Environment\\HW1\\Studyproject_HW1\\stan\\vanilla_rsa.stan to exe file C:\\Users\\User\\Proton Drive\\maa_ng0h\\My files\\C. Work\\2E Modeling Communication and Abstraction\\Environment\\HW1\\Studyproject_HW1\\stan\\vanilla_rsa.exe\n",
      "18:45:31 - cmdstanpy - INFO - compiled model executable: C:\\Users\\User\\Proton Drive\\maa_ng0h\\My files\\C. Work\\2E Modeling Communication and Abstraction\\Environment\\HW1\\Studyproject_HW1\\stan\\vanilla_rsa.exe\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "stan_file = os.path.join('stan', 'vanilla_rsa.stan')\n",
    "rsa_model = CmdStanModel(stan_file=stan_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e682e5eca56c67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:16:47.445896Z",
     "start_time": "2025-11-10T13:16:47.442173Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'big_red'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# cost function\u001b[39;00m\n\u001b[32m     25\u001b[39m cost_dict = {\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbig\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.0\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msmall\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.0\u001b[39m,\n\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mblue\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.0\u001b[39m,\n\u001b[32m     29\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mred\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.0\u001b[39m,\n\u001b[32m     30\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m default_cost = np.array([\u001b[43mcost_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mutterance\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m utterance \u001b[38;5;129;01min\u001b[39;00m utterances])\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# new cost functions\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcost_by_letters\u001b[39m(utterance, per_letter=\u001b[32m0.1\u001b[39m):\n",
      "\u001b[31mKeyError\u001b[39m: 'big_red'"
     ]
    }
   ],
   "source": [
    "# define input data\n",
    "states = [\"big_blue\", \"big_red\", \"small_blue\"]\n",
    "short_utterances = [\n",
    "    \"big\", \"small\", \"blue\", \"red\"\n",
    "]\n",
    "utterances = [\n",
    "    \"big\", \"small\", \"blue\", \"red\", \"big_red\", \"big_blue\", \"small_blue\"\n",
    "]\n",
    "n_states = len(states)\n",
    "n_utterances   = len(utterances)\n",
    "\n",
    "# build meaning_matrix[u, s]\n",
    "meaning_matrix = np.zeros((n_utterances, n_states), dtype=int)\n",
    "for u, utterance in enumerate(utterances):\n",
    "    for s, state in enumerate(states):\n",
    "        # literal meaning maps to true iff the utterance string appears in the state string\n",
    "        # Stan cannot handle booleans, so we need to work with integers here\n",
    "        meaning_matrix[u, s] = int(utterance in state)\n",
    "\n",
    "# parameters - change them here\n",
    "alpha = 1.0\n",
    "cost_weight = 1.0\n",
    "\n",
    "# cost function\n",
    "cost_dict = {\n",
    "    \"big\": 0.0,\n",
    "    \"small\": 0.0,\n",
    "    \"blue\": 0.0,\n",
    "    \"red\": 0.0,\n",
    "}\n",
    "default_cost = np.array([cost_dict[utterance] for utterance in short_utterances])\n",
    "\n",
    "# new cost functions\n",
    "def cost_by_letters(utterance, per_letter=0.1):\n",
    "    letters = sum(1 for ch in utterance if ch.isalpha())\n",
    "    return letters * per_letter\n",
    "\n",
    "def cost_by_underscores(utterance, per_underscore=.0):\n",
    "    return utterance.count('_') * per_underscore\n",
    "\n",
    "cost = np.array([cost_by_underscores(utterance) for utterance in utterances])\n",
    "\n",
    "# prepare Stan data as dictionary\n",
    "stan_data = {\n",
    "    \"S\": n_states,\n",
    "    \"U\": n_utterances,\n",
    "    \"meaning_matrix\": meaning_matrix.tolist(), # Stan cannot handle numpy arrays       \n",
    "    \"cost\": cost.tolist(),                     # or dictionaries\n",
    "    \"alpha\": alpha,\n",
    "    \"cost_weight\": cost_weight\n",
    "}\n",
    "\n",
    "fit = rsa_model.sample(stan_data, show_console=True, chains=1, iter_warmup=0, adapt_engaged=False, iter_sampling=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b0f71db90a544a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "L0_draws = fit.stan_variable('L0')\n",
    "S1_draws = fit.stan_variable('S1')\n",
    "\n",
    "L0 = L0_draws[0]\n",
    "S1 = S1_draws[0]\n",
    "\n",
    "df_L0 = pd.DataFrame(L0, index=utterances, columns=states)\n",
    "df_S1 = pd.DataFrame(S1, index=states, columns=utterances)\n",
    "\n",
    "print(\"L0:\")\n",
    "print(df_L0.to_string(float_format=\"{:.2f}\".format))\n",
    "print(\"S1:\")\n",
    "print(df_S1.to_string(float_format=\"{:.2f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da94ccb88c8848",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3.1.3) Are the outputs in line with what you would expect given your knowledge about pragmatic communication and overinformative referring expressions?\n",
    "Add complex utterances to the model (i.e., utterance consisting of a size and color adjective) and inspect the output again. The meaning of a complex two-word utterance is defined with intuitive intersective semantics: $$\\mathcal{L}(u_{\\text{complex}}, o)=\\mathcal{L}(u_{\\text{size}},o)\\times\\mathcal{L}(u_{\\text{color}},o)$$ (6 points)\n",
    "\n",
    "    It makes sense, especially in S1 that the utility of the characteristic that is unique about a state (e.g. \"red\" in \"big_red\") is higher, as there is no ambiguity there.\n",
    "    When adding a complex utterance where its consituence are found in the states but not in that combination, it shows NaN in L0 and then S1 is filled with NaN everywhere. \n",
    "    If we add the three complex utterances that fully describe the states, we can observe that in S1, the utility of non-complex utterances is equal to or lower than the complex utterances. This makes intuitive sense if there is no cost of length, as it is more precise in referring to a state.\n",
    "\n",
    "3.1.4) Play around with the rationality and cost weight parameters. How do they affect the model output? (4 points)\n",
    "\n",
    "    The lower rationality, the more uniform are the values distributed. \n",
    "    The higher the costs, the more non-complex statements are valued.\n",
    "\n",
    "3.1.5) Adapt the utterance cost in a way that achieves a preference for overinformative referring expressions. (2 points)\n",
    "\n",
    "    Add minus to \"per_letter\" or \"per_underscore\"\n",
    "    I \n",
    "3.1.6) Adapt the utterance cost in a way that seems most natural to you. (1 point)\n",
    "\n",
    "    per word cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2508f36e391df9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2) Relaxed semantics (20 points)\n",
    "It seems that our intuitions do not align well with the model. Let's use continuous rather than boolean semantics to see whether this can solve our problem. In the following, you need to adapt the RSA model and input data in a way that implements continuous semantics. The only change will be that the lexicon, or meaning matrix, should return real values instead of true or false: $$\\mathcal{L}(u,o)\\in [0,1] \\subset \\mathbb{R}$$\n",
    "This approach captures the intuition that an object is not unambiguously big or blue, but rather that objects can count as big or blue to varying degrees.\n",
    "\n",
    "3.2.1) Build a meaning matrix that captures the relaxed semantics with two new parameters size_semantics $x_\\text{size}$ and color_semantics $x_\\text{color}$. When an object $o$ is in the extension of a size adjective under the Boolean semantics defined above, take $\\mathcal{L}(u,o)=x_\\text{size}$, else $\\mathcal{L}(u,o)=1-x_\\text{size}$. The semantics are defined analogously for color. (6 points)\n",
    "3.2.2) Run the model with alpha = 30, size_semantics = 0.8 and color_semantics = 0.99. Inspect the model outputs. (4 points)\n",
    "3.2.3) Visualize the results of varying values for size_semantics and color_semantics, pit them against each other and interpret them. (6 points)\n",
    "3.2.4) Van Gompel et al. (2019) found that speakers use overinformative referring expressions in about 80% of the trials that look like the one above, where size is sufficient to mention. What about contexts where color is sufficient to mention? Construct a context where color is sufficient to mention and interpret the output. (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb96781dbd981a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3) Model evaluation by comparison to experiment data (42 points)\n",
    "3.3.1) Create a new file `sem_rsa.stan`. Adapt the vanilla RSA model in a way that allows you to infer all free parameters instead of specifying them beforehand. Condition the model on the observed production data (`data/data_exp1.csv`) and integrate over the free parameters. Preprocess the observed data in a way that you see fit for the modeling purpose. Assume uniform priors for each parameter. Use the generated quantities block in your Stan model to generate the posterior predictive distribution (read up [Stan documentation](https://mc-stan.org/docs/stan-users-guide/posterior-prediction.html) for this). Choose an appropriate number of iterations for warm up and sampling from the posterior. (16 points)\n",
    "3.3.2) Diagnose the model convergence and take actions if necessary. (4 points)\n",
    "3.3.3) Interpret a summary of the fitted model. (6 points)\n",
    "3.3.4) Correlate the model's posterior predictive distribution for overinformative utterance probabilities with the empirical data to assess and interpret model fit to the data. (8 points)\n",
    "3.3.5) Bonus: Introduce separate cost parameters for size and color. (6 bonus points)\n",
    "3.3.6) Interpret and discuss your findings. (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a869a3bf1dd707",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## X) Reflection (no points, but mandatory)\n",
    "\n",
    "Reflect on your group work. What went well? What did not go well?\n",
    "\n",
    "Please note down the group members' team roles anonymously and reflect on how you filled this role."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CommunicationAbstraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
