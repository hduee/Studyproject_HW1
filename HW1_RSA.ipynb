{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:13:57.889461Z",
     "start_time": "2025-11-10T13:13:57.886996Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from cmdstanpy import CmdStanModel\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Bayesian Cognitive and Rational Speech Act models\n",
    "\n",
    "This homework assignment is to be completed in groups. It is due on November 27, 2025 (midnight). Please upload *all files you created or modified* to the homework folder of your group in studIP.\n",
    "\n",
    "Group number:\n",
    "\n",
    "Names:\n",
    "\n",
    "*General note: It is permitted to use AI tools for coding. Please refer to the uploaded manual `AI_Tools_Guidelines` for recommended ways how to use AI to advance your studies in a way that supports your learning. That means that you should not be satisfied if an AI tool hands you a working version of your code, but that you should put in effort to understand how exactly the problem is solved. Another note of caution: What might work for large programming languages like Python, does not necessarily work for Stan. Check your code carefully and do NOT blindly trust AI.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "During the past weeks, you have learned how Bayesian inference works and how it can be used in Bayesian cognitive models. You also learned about a specific type of Bayesian models that can be used to model pragmatic language understanding and production, the Rational Speech Act models. The goal of this homework assignment is for you to learn how to implement Bayesian models in Stan and, specifically, how to implement RSA models in Stan. A special focus will be on the different use cases and evaluation methods of RSA models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Stan modeling (8 points)\n",
    "\n",
    "1.1) In the file `simple_model.stan`, you will find a simple Stan model. Describe its implementation, relating it to the knowledge you gained about the conventions for coding models in Stan. (4 points)\n",
    "\n",
    "- The data block defines the observed data: we have N trials, and for each trial we know the semantic similarity, the phonological similarity, and the response time.\n",
    "- In the transformed data block, the response times are log-transformed, so the model works on log_rt. This makes the normality assumption more reasonable, because raw response times are positive and often skewed.\n",
    "- In the parameters block, the model estimates:\n",
    "    - alpha (intercept),\n",
    "    - beta_sem (effect of semantic similarity),\n",
    "    - beta_phon (effect of phonological similarity),\n",
    "    - sigma_rt (residual standard deviation, constrained to be positive).\n",
    "    - Each of these parameters has a weakly informative normal prior.\n",
    "- In the model block, each log_rt[i] is assumed to follow a normal distribution with mean alpha + beta_sem * semantic_similarity[i] + beta_phon * phonological_similarity[i] and standard deviation sigma_rt. So the two similarity measures linearly predict log response time.\n",
    "- In the generated quantities block:\n",
    "    - log_likelihood[i] stores the pointwise log density for each trial,\n",
    "    - log_rt_ppd[i] stores posterior-predictive draws on the log scale,\n",
    "    - rt_ppd[i] exponentiates those draws back to the original time scale, giving predicted response times for posterior predictive checks.\n",
    "\n",
    "Essentially, it’s a Bayesian linear regression model that predicts how fast someone responds based on how semantically and phonologically related the word and context are.\n",
    "\n",
    "1.2) You will notice that the model does not compile. Fix the problems and explain what you did. (4 points)\n",
    "\n",
    "    The model compiles:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Bayesian cognitive models (10 points)\n",
    "Think of a use case for `simple_model.stan` in the scope of Bayesian cognitive modeling. Describe the model while answering the following questions:\n",
    "\n",
    "2.1) What cognitive capacity can be explained by this model? (2 points)\n",
    "\n",
    "- The model describes lexical access in tasks like visual lexical decision or picture naming: how quickly a person can retrieve the correct word form when given semantic and phonological cues from a context.\n",
    "- The predictors capture two psychologically meaningful influences:\n",
    "    - semantic priming: effect of meaning similarity,\n",
    "    - phonological priming: effect of sound similarity,\n",
    "- and the strength of these influences changes the observed response times.\n",
    "\n",
    "2.2) What is the purpose and function of this model? (3 points)\n",
    "\n",
    "- The model explains how semantic and phonological similarity jointly produce the distribution of RTs we observe.\n",
    "- By fitting this Bayesian regression, we can:\n",
    "    - estimate how much semantic and phonological similarity speed up or slow down retrieval,\n",
    "    - quantify priming effects at the level of participants or conditions,\n",
    "    - generate posterior predictive RT distributions for new items or contexts,\n",
    "    - compare hypotheses (e.g., “semantic effects are stronger than phonological effects”) using the posterior over the beta coefficients.\n",
    "\n",
    "2.3) At which level of analysis does it model this cognitive capacity and why? (3 points)\n",
    "\n",
    "- The Stan model is a computational model in Marr’s framework. It specifies what mapping the cognitive system implements: given inputs (semantic similarity and phonological similarity), it defines a probability distribution over response times.\n",
    "- The code does not specify an algorithm or representation format for lexical access (no activation dynamics, no specific retrieval steps, no architecture), so it is not an algorithmic-level model. And it does not mention neural structures or hardware, so it is not an implementational-level model either. It remains at the abstract, probabilistic computational-level description of the capacity “map semantic/phonological similarity to response-time distributions.”\n",
    "\n",
    "\n",
    "Overall coherence gives another 2 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) RSA modeling (82 points)\n",
    "The purpose of the following model is to explain the use of overinformative referring expressions in pragmatic communication. In referential communication, the speaker’s task is to produce a referring expression that allows a listener to identify the target in the context. Consider the context below, where the target is the small blue pin. A referring expression including a size adjective (the small pin) is strictly speaking sufficient for uniquely establishing reference to the target, yet speakers often “overmodify” with color, producing referring expressions like the small blue pin. This overmodification phenomenon is what the model is intended to capture.\n",
    "\n",
    "<img src=\"img/size-sufficient.png\" width=\"400\"/>\n",
    "\n",
    "### 3.1) Vanilla RSA (20 points)\n",
    "In the file `vanilla_rsa.stan`, you find an RSA model of the production of referring expressions, based on the vanilla RSA model of Frank & Goodman (2012) that we discussed in class.\n",
    "\n",
    "3.1.1) Provide informative comments in the file `vanilla_rsa.stan`. (4 points)\n",
    "\n",
    "3.1.2) You will notice that the parameters and model blocks are empty. Why is that? Go through the following code and inspect the model's behavior. Look at the stan variables that are included in the fitted model. (3 points)\n",
    "\n",
    "    There are no parameters and no defined model, as we are not doing Bayesian sampling. We are not trying to estimate any values but rather arrive at our results deterministically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T09:58:12.006198Z",
     "start_time": "2025-11-11T09:58:11.938342Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:34:08 - cmdstanpy - INFO - compiling stan file /tmp/tmpaoikg90r/tmpe8xxjss6.stan to exe file /home/laura/Documents/Uni/Communication and Abstraction/HW_I/Studyproject_HW1/stan/vanilla_rsa\n",
      "20:34:15 - cmdstanpy - INFO - compiled model executable: /home/laura/Documents/Uni/Communication and Abstraction/HW_I/Studyproject_HW1/stan/vanilla_rsa\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "stan_file = os.path.join('stan', 'vanilla_rsa.stan')\n",
    "rsa_model = CmdStanModel(stan_file=stan_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:16:47.445896Z",
     "start_time": "2025-11-10T13:16:47.442173Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:42:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:42:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:42:10 - cmdstanpy - ERROR - Chain [1] error: code '1' Operation not permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain [1] method = sample (Default)\n",
      "Chain [1] sample\n",
      "Chain [1] num_samples = 1\n",
      "Chain [1] num_warmup = 0\n",
      "Chain [1] save_warmup = 0 (Default)\n",
      "Chain [1] thin = 1 (Default)\n",
      "Chain [1] adapt\n",
      "Chain [1] engaged = 0\n",
      "Chain [1] gamma = 0.050000000000000003 (Default)\n",
      "Chain [1] delta = 0.80000000000000004 (Default)\n",
      "Chain [1] kappa = 0.75 (Default)\n",
      "Chain [1] t0 = 10 (Default)\n",
      "Chain [1] init_buffer = 75 (Default)\n",
      "Chain [1] term_buffer = 50 (Default)\n",
      "Chain [1] window = 25 (Default)\n",
      "Chain [1] algorithm = hmc (Default)\n",
      "Chain [1] hmc\n",
      "Chain [1] engine = nuts (Default)\n",
      "Chain [1] nuts\n",
      "Chain [1] max_depth = 10 (Default)\n",
      "Chain [1] metric = diag_e (Default)\n",
      "Chain [1] metric_file =  (Default)\n",
      "Chain [1] stepsize = 1 (Default)\n",
      "Chain [1] stepsize_jitter = 0 (Default)\n",
      "Chain [1] num_chains = 1 (Default)\n",
      "Chain [1] id = 1 (Default)\n",
      "Chain [1] data\n",
      "Chain [1] file = C:\\Users\\Hannes\\AppData\\Local\\Temp\\tmpcbiqvtis\\lpnhl_cv.json\n",
      "Chain [1] init = 2 (Default)\n",
      "Chain [1] random\n",
      "Chain [1] seed = 98883\n",
      "Chain [1] output\n",
      "Chain [1] file = C:\\Users\\Hannes\\AppData\\Local\\Temp\\tmpcbiqvtis\\vanilla_rsa_relaxed_semanticskoavfas7\\vanilla_rsa_relaxed_semantics-20251126184210.csv\n",
      "Chain [1] diagnostic_file =  (Default)\n",
      "Chain [1] refresh = 100 (Default)\n",
      "Chain [1] sig_figs = -1 (Default)\n",
      "Chain [1] profile_file = profile.csv (Default)\n",
      "Chain [1] num_threads = 1 (Default)\n",
      "Chain [1] \n",
      "Chain [1] Exception: variable does not exist; processing stage=data initialization; variable name=x_size; base type=double (in 'vanilla_rsa_relaxed_semantics.stan', line 18, column 2 to column 31)\n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error during sampling:\nException: variable does not exist; processing stage=data initialization; variable name=x_size; base type=double (in 'vanilla_rsa_relaxed_semantics.stan', line 18, column 2 to column 31)\nCommand and output files:\nRunSet: chains=1, chain_ids=[1], num_processes=1\n cmd (chain 1):\n\t['C:\\\\Users\\\\Hannes\\\\Desktop\\\\Uni_Osna\\\\studienprojekt\\\\github\\\\Studyproject_HW1\\\\stan\\\\vanilla_rsa_relaxed_semantics.exe', 'id=1', 'random', 'seed=98883', 'data', 'file=C:\\\\Users\\\\Hannes\\\\AppData\\\\Local\\\\Temp\\\\tmpcbiqvtis\\\\lpnhl_cv.json', 'output', 'file=C:\\\\Users\\\\Hannes\\\\AppData\\\\Local\\\\Temp\\\\tmpcbiqvtis\\\\vanilla_rsa_relaxed_semanticskoavfas7\\\\vanilla_rsa_relaxed_semantics-20251126184210.csv', 'method=sample', 'num_samples=1', 'num_warmup=0', 'algorithm=hmc', 'adapt', 'engaged=0']\n retcodes=[1]\n per-chain output files (showing chain 1 only):\n csv_file:\n\tC:\\Users\\Hannes\\AppData\\Local\\Temp\\tmpcbiqvtis\\vanilla_rsa_relaxed_semanticskoavfas7\\vanilla_rsa_relaxed_semantics-20251126184210.csv\n console_msgs (if any):\n\tC:\\Users\\Hannes\\AppData\\Local\\Temp\\tmpcbiqvtis\\vanilla_rsa_relaxed_semanticskoavfas7\\vanilla_rsa_relaxed_semantics-20251126184210_0-stdout.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# prepare Stan data as dictionary\u001b[39;00m\n\u001b[32m     44\u001b[39m stan_data = {\n\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m: n_states,\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mU\u001b[39m\u001b[33m\"\u001b[39m: n_utterances,\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcost_weight\u001b[39m\u001b[33m\"\u001b[39m: cost_weight\n\u001b[32m     51\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m fit = \u001b[43mrsa_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstan_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_console\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchains\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_warmup\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapt_engaged\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_sampling\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\studyproject\\Lib\\site-packages\\cmdstanpy\\model.py:1180\u001b[39m, in \u001b[36mCmdStanModel.sample\u001b[39m\u001b[34m(self, data, chains, parallel_chains, threads_per_chain, seed, chain_ids, inits, iter_warmup, iter_sampling, save_warmup, thin, max_treedepth, metric, step_size, adapt_engaged, adapt_delta, adapt_init_phase, adapt_metric_window, adapt_step_size, fixed_param, output_dir, sig_figs, save_latent_dynamics, save_profile, show_progress, show_console, refresh, time_fmt, timeout, force_one_process_per_chain, inv_metric)\u001b[39m\n\u001b[32m   1175\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m show_console:\n\u001b[32m   1176\u001b[39m         msg += (\n\u001b[32m   1177\u001b[39m             \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mConsider re-running with show_console=True if the\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1178\u001b[39m             \u001b[33m'\u001b[39m\u001b[33m above output is unclear!\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1179\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[32m   1181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m errors:\n\u001b[32m   1182\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mNon-fatal error during sampling:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merrors\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: Error during sampling:\nException: variable does not exist; processing stage=data initialization; variable name=x_size; base type=double (in 'vanilla_rsa_relaxed_semantics.stan', line 18, column 2 to column 31)\nCommand and output files:\nRunSet: chains=1, chain_ids=[1], num_processes=1\n cmd (chain 1):\n\t['C:\\\\Users\\\\Hannes\\\\Desktop\\\\Uni_Osna\\\\studienprojekt\\\\github\\\\Studyproject_HW1\\\\stan\\\\vanilla_rsa_relaxed_semantics.exe', 'id=1', 'random', 'seed=98883', 'data', 'file=C:\\\\Users\\\\Hannes\\\\AppData\\\\Local\\\\Temp\\\\tmpcbiqvtis\\\\lpnhl_cv.json', 'output', 'file=C:\\\\Users\\\\Hannes\\\\AppData\\\\Local\\\\Temp\\\\tmpcbiqvtis\\\\vanilla_rsa_relaxed_semanticskoavfas7\\\\vanilla_rsa_relaxed_semantics-20251126184210.csv', 'method=sample', 'num_samples=1', 'num_warmup=0', 'algorithm=hmc', 'adapt', 'engaged=0']\n retcodes=[1]\n per-chain output files (showing chain 1 only):\n csv_file:\n\tC:\\Users\\Hannes\\AppData\\Local\\Temp\\tmpcbiqvtis\\vanilla_rsa_relaxed_semanticskoavfas7\\vanilla_rsa_relaxed_semantics-20251126184210.csv\n console_msgs (if any):\n\tC:\\Users\\Hannes\\AppData\\Local\\Temp\\tmpcbiqvtis\\vanilla_rsa_relaxed_semanticskoavfas7\\vanilla_rsa_relaxed_semantics-20251126184210_0-stdout.txt"
     ]
    }
   ],
   "source": [
    "# define input data\n",
    "states = [\"big_blue\", \"big_red\", \"small_blue\"]\n",
    "short_utterances = [\n",
    "    \"big\", \"small\", \"blue\", \"red\"\n",
    "]\n",
    "utterances = [\n",
    "    \"big\", \"small\", \"blue\", \"red\", \"big_red\", \"big_blue\", \"small_blue\"\n",
    "]\n",
    "n_states = len(states)\n",
    "n_utterances   = len(utterances)\n",
    "\n",
    "# build meaning_matrix[u, s]\n",
    "meaning_matrix = np.zeros((n_utterances, n_states), dtype=int)\n",
    "for u, utterance in enumerate(utterances):\n",
    "    for s, state in enumerate(states):\n",
    "        # literal meaning maps to true iff the utterance string appears in the state string\n",
    "        # Stan cannot handle booleans, so we need to work with integers here\n",
    "        meaning_matrix[u, s] = int(utterance in state)\n",
    "\n",
    "# parameters - change them here\n",
    "alpha = 1.0\n",
    "cost_weight = 1.0\n",
    "\n",
    "# cost function\n",
    "cost_dict = {\n",
    "    \"big\": 0.0,\n",
    "    \"small\": 0.0,\n",
    "    \"blue\": 0.0,\n",
    "    \"red\": 0.0,\n",
    "}\n",
    "default_cost = np.array([cost_dict[utterance] for utterance in short_utterances])\n",
    "\n",
    "# new cost functions\n",
    "def cost_by_letters(utterance, per_letter=0.1):\n",
    "    letters = sum(1 for ch in utterance if ch.isalpha())\n",
    "    return letters * per_letter\n",
    "\n",
    "def cost_by_underscores(utterance, per_underscore=.0):\n",
    "    return utterance.count('_') * per_underscore\n",
    "\n",
    "cost = np.array([cost_by_underscores(utterance) for utterance in utterances])\n",
    "\n",
    "# prepare Stan data as dictionary\n",
    "stan_data = {\n",
    "    \"S\": n_states,\n",
    "    \"U\": n_utterances,\n",
    "    \"meaning_matrix\": meaning_matrix.tolist(), # Stan cannot handle numpy arrays       \n",
    "    \"cost\": cost.tolist(),                     # or dictionaries\n",
    "    \"alpha\": alpha,\n",
    "    \"cost_weight\": cost_weight\n",
    "}\n",
    "\n",
    "fit = rsa_model.sample(stan_data, show_console=True, chains=1, iter_warmup=0, adapt_engaged=False, iter_sampling=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0:\n",
      "            big_blue  big_red  small_blue\n",
      "big             0.50     0.50        0.00\n",
      "small           0.00     0.00        1.00\n",
      "blue            0.50     0.00        0.50\n",
      "red             0.00     1.00        0.00\n",
      "big_red         0.00     1.00        0.00\n",
      "big_blue        1.00     0.00        0.00\n",
      "small_blue      0.00     0.00        1.00\n",
      "S1:\n",
      "            big  small  blue  red  big_red  big_blue  small_blue\n",
      "big_blue   0.25   0.00  0.25 0.00     0.00      0.50        0.00\n",
      "big_red    0.20   0.00  0.00 0.40     0.40      0.00        0.00\n",
      "small_blue 0.00   0.40  0.20 0.00     0.00      0.00        0.40\n"
     ]
    }
   ],
   "source": [
    "\n",
    "L0_draws = fit.stan_variable('L0')\n",
    "S1_draws = fit.stan_variable('S1')\n",
    "\n",
    "L0 = L0_draws[0]\n",
    "S1 = S1_draws[0]\n",
    "\n",
    "df_L0 = pd.DataFrame(L0, index=utterances, columns=states)\n",
    "df_S1 = pd.DataFrame(S1, index=states, columns=utterances)\n",
    "\n",
    "print(\"L0:\")\n",
    "print(df_L0.to_string(float_format=\"{:.2f}\".format))\n",
    "print(\"S1:\")\n",
    "print(df_S1.to_string(float_format=\"{:.2f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.3) Are the outputs in line with what you would expect given your knowledge about pragmatic communication and overinformative referring expressions?\n",
    "Add complex utterances to the model (i.e., utterance consisting of a size and color adjective) and inspect the output again. The meaning of a complex two-word utterance is defined with intuitive intersective semantics: $$\\mathcal{L}(u_{\\text{complex}}, o)=\\mathcal{L}(u_{\\text{size}},o)\\times\\mathcal{L}(u_{\\text{color}},o)$$ (6 points)\n",
    "\n",
    "    It makes sense, especially in S1 that the utility of the characteristic that is unique about a state (e.g. \"red\" in \"big_red\") is higher, as there is no ambiguity there.\n",
    "    When adding a complex utterance where its consituence are found in the states but not in that combination, it shows NaN in L0 and then S1 is filled with NaN everywhere. \n",
    "    If we add the three complex utterances that fully describe the states, we can observe that in S1, the utility of non-complex utterances is equal to or lower than the complex utterances. This makes intuitive sense if there is no cost of length, as it is more precise in referring to a state.\n",
    "\n",
    "3.1.4) Play around with the rationality and cost weight parameters. How do they affect the model output? (4 points)\n",
    "\n",
    "    The lower rationality, the more uniform are the values distributed. \n",
    "    The higher the costs, the more non-complex statements are valued.\n",
    "\n",
    "3.1.5) Adapt the utterance cost in a way that achieves a preference for overinformative referring expressions. (2 points)\n",
    "\n",
    "    Add minus to \"per_letter\" or \"per_underscore\"\n",
    "    I \n",
    "3.1.6) Adapt the utterance cost in a way that seems most natural to you. (1 point)\n",
    "\n",
    "    per word cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Relaxed semantics (20 points)\n",
    "It seems that our intuitions do not align well with the model. Let's use continuous rather than boolean semantics to see whether this can solve our problem. In the following, you need to adapt the RSA model and input data in a way that implements continuous semantics. The only change will be that the lexicon, or meaning matrix, should return real values instead of true or false: $$\\mathcal{L}(u,o)\\in [0,1] \\subset \\mathbb{R}$$\n",
    "This approach captures the intuition that an object is not unambiguously big or blue, but rather that objects can count as big or blue to varying degrees.\n",
    "\n",
    "3.2.1) Build a meaning matrix that captures the relaxed semantics with two new parameters size_semantics $x_\\text{size}$ and color_semantics $x_\\text{color}$. When an object $o$ is in the extension of a size adjective under the Boolean semantics defined above, take $\\mathcal{L}(u,o)=x_\\text{size}$, else $\\mathcal{L}(u,o)=1-x_\\text{size}$. The semantics are defined analogously for color. (6 points)\n",
    "3.2.2) Run the model with alpha = 30, size_semantics = 0.8 and color_semantics = 0.99. Inspect the model outputs. (4 points)\n",
    "3.2.3) Visualize the results of varying values for size_semantics and color_semantics, pit them against each other and interpret them. (6 points)\n",
    "3.2.4) Van Gompel et al. (2019) found that speakers use overinformative referring expressions in about 80% of the trials that look like the one above, where size is sufficient to mention. What about contexts where color is sufficient to mention? Construct a context where color is sufficient to mention and interpret the output. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "stan_file = os.path.join('stan', 'vanilla_rsa_relaxed_semantics.stan')\n",
    "rsa_model = CmdStanModel(stan_file=stan_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# check utterance category\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m utterance \u001b[38;5;129;01min\u001b[39;00m size_utterances:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     x = \u001b[43mx_size\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m utterance \u001b[38;5;129;01min\u001b[39;00m color_utterances:\n\u001b[32m     25\u001b[39m     x = x_color\n",
      "\u001b[31mNameError\u001b[39m: name 'x_size' is not defined"
     ]
    }
   ],
   "source": [
    "# This is the code for exercise 3.2, adapted from the code for exercise 3.1.\n",
    "\n",
    "# define input data\n",
    "states = [\"big_blue\", \"big_red\", \"small_blue\"]\n",
    "utterances = [\n",
    "    \"big\", \"small\", \"blue\", \"red\"\n",
    "]\n",
    "n_states = len(states)\n",
    "n_utterances   = len(utterances)\n",
    "\n",
    "# introduce size/color categorization\n",
    "size_utterances  = [\"big\", \"small\"]\n",
    "color_utterances = [\"blue\", \"red\"]\n",
    "\n",
    "# build meaning_matrix[u, s]\n",
    "meaning_matrix = np.zeros((n_utterances, n_states))\n",
    "for u, utterance in enumerate(utterances):\n",
    "    for s, state in enumerate(states):\n",
    "        # check if object is in the extension of an adjective under Boolean semantics\n",
    "        extension = utterance in state\n",
    "        # check utterance category\n",
    "        if utterance in size_utterances:\n",
    "            x = x_size\n",
    "        elif utterance in color_utterances:\n",
    "            x = x_color\n",
    "        # if object is in the extension of an adjective, take L(u,o) = x_size/color, else L(u,o) = 1 - x_size/color\n",
    "        # e.g. \"big\" refers to big objects with probabiliy x and small objects with probability 1-x\n",
    "        meaning_matrix[u, s] = x if extension else (1 - x)\n",
    "\n",
    "# parameters - change them here\n",
    "alpha = 30\n",
    "cost_weight = 1.0\n",
    "\n",
    "# cost function\n",
    "cost_dict = {\n",
    "    \"big\": 0.0,\n",
    "    \"small\": 0.0,\n",
    "    \"blue\": 0.0,\n",
    "    \"red\": 0.0,\n",
    "}\n",
    "cost = np.array([cost_dict[utterance] for utterance in utterances])\n",
    "\n",
    "# prepare Stan data as dictionary\n",
    "stan_data = {\n",
    "    \"S\": n_states,\n",
    "    \"U\": n_utterances,\n",
    "    \"meaning_matrix\": meaning_matrix.tolist(), # Stan cannot handle numpy arrays       \n",
    "    \"cost\": cost.tolist(),                     # or dictionaries\n",
    "    \"alpha\": alpha,\n",
    "    \"cost_weight\": cost_weight,\n",
    "    \"x_size\": x_size,\n",
    "    \"x_color\": x_color\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.2.2: Run the model with alpha = 30, size_semantics = 0.8 and color_semantics = 0.99. Inspect the model outputs.\n",
    "\n",
    "# We observe that color behaves like in non-relaxed semantics. For L0, P(\"big_red\"|\"red\") = 1.0, reflecting that the color word\n",
    "# practically determines that a pin of its color is chosen. We say \"practically\" because 0.99 is of course not 100%, but in\n",
    "# practice we get a 1.0 probability. Again for L0, P(\"big_blue\"|\"blue\") = 0.50. When the color word simply deterministically \n",
    "# decides that a pin of the stated color is to be chosen, the pins of the stated color are the only, equally viable candidates.\n",
    "# S1 behaves analoguously. The size parameter is large enough to behave similarly as well.\n",
    "\n",
    "# The number of chains should not make a difference with fixed parameters.\n",
    "# For iter_warmup and iter_sampling, it is apparently best practice in a lot of cases to simply choose 500 because\n",
    "# models will converge between 200 and 400. Going above 200 made absolutely no difference for us though, using a forward model.\n",
    "fit = rsa_model.sample(stan_data, show_console=True, chains=1, iter_warmup=200, adapt_engaged=True, iter_sampling=200)\n",
    "# fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results for exercise 3.2.2. \n",
    "# Exact same code as we used for exercise 3.1\n",
    "\n",
    "L0_draws = fit.stan_variable('L0')\n",
    "S1_draws = fit.stan_variable('S1')\n",
    "\n",
    "L0 = L0_draws[0]\n",
    "S1 = S1_draws[0]\n",
    "\n",
    "df_L0 = pd.DataFrame(L0, index=utterances, columns=states)\n",
    "df_S1 = pd.DataFrame(S1, index=states, columns=utterances)\n",
    "\n",
    "print(\"L0:\")\n",
    "print(df_L0.to_string(float_format=\"{:.2f}\".format))\n",
    "print(\"S1:\")\n",
    "print(df_S1.to_string(float_format=\"{:.2f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.2.3\n",
    "# The heat map generated by this cell shows P(\"blue\"|\"big_blue\") for S1 given different size and color parameters.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This function was added (out of frustration) because the heat map just would not work at first.\n",
    "# It is (as a beginner) not always intuitive how indexing works in Stan.\n",
    "# The first versions of this cell generated heat maps that were only one color.\n",
    "# Printing out the matrix showed that we read the indices in a confused manner. \n",
    "# This is possibly bloated, but it makes the code below more readable. \n",
    "def extract_matrix(samples, prefix, n_row, n_col):\n",
    "    M = np.zeros((n_row, n_col))\n",
    "    for r in range(1, n_row + 1):\n",
    "        for c in range(1, n_col + 1):\n",
    "            col = f\"{prefix}[{r},{c}]\"\n",
    "            M[r - 1, c - 1] = samples[col].mean()\n",
    "    return M\n",
    "\n",
    "# We want to run the model for different values. This function will later be called from inside a loop to accomplish just that.\n",
    "def run_rsa_for(x_size, x_color, rsa_model, base_stan_data):\n",
    "    # The base data is copied to avoid potential side effects.\n",
    "    stan_data = base_stan_data.copy()\n",
    "    \n",
    "    # This is familiar from the code for exercise 3.2.2.\n",
    "    # The different values we want to compare will be plugged in here.\n",
    "    stan_data[\"x_size\"] = x_size\n",
    "    stan_data[\"x_color\"] = x_color\n",
    "\n",
    "    # Same setup as in exercise 3.2.2\n",
    "    fit = rsa_model.sample(\n",
    "        stan_data,\n",
    "        show_console=False,\n",
    "        chains=1,            \n",
    "        iter_warmup=200,\n",
    "        iter_sampling=200,\n",
    "        adapt_engaged=True\n",
    "    )\n",
    "    \n",
    "    # Extract transformed parameters\n",
    "    samples = fit.draws_pd()\n",
    "    \n",
    "    # Return mean values of L0 and S1.\n",
    "    # Printing these out we get \"L0[1,2]\", \"S1[2,3]\", and so on and so forth.\n",
    "    # This was helpful for troubleshooting, like the Stan manual suggests.\n",
    "    L0_cols = [c for c in samples.columns if c.startswith(\"L0[\")]\n",
    "    S1_cols = [c for c in samples.columns if c.startswith(\"S1[\")]\n",
    "\n",
    "    U = stan_data[\"U\"]\n",
    "    S = stan_data[\"S\"]\n",
    "\n",
    "    L0_mean = extract_matrix(samples, \"L0\", U, S)   # U × S\n",
    "    S1_mean = extract_matrix(samples, \"S1\", S, U)   # S × U\n",
    "\n",
    "    return L0_mean, S1_mean\n",
    "\n",
    "# We will try the values 0.1, 02..0.9, 1.0 for size and color.\n",
    "# This simply seemed like the obvious choice, and it results in an interesting visualization.\n",
    "x_size_vals = np.linspace(0.1, 1.0, 10)\n",
    "x_color_vals = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Here we store the results for visualization.\n",
    "results = {}\n",
    "\n",
    "# This is the loop announced above that runs the model for each value-pair.\n",
    "for xs in x_size_vals:\n",
    "    for xc in x_color_vals:\n",
    "\n",
    "        # Rebuild meaning matrix\n",
    "        # This is the same code as for exercise 3.2.2, except that it includes the variables xs and xc.\n",
    "        # Otherwise we would not actually iterate over different value-pairs.\n",
    "        meaning_matrix = np.zeros((n_utterances, n_states))\n",
    "        for u, utterance in enumerate(utterances):\n",
    "            for s, state in enumerate(states):\n",
    "                extension = utterance in state\n",
    "                if utterance in size_utterances:\n",
    "                    x = xs\n",
    "                elif utterance in color_utterances:\n",
    "                    x = xc\n",
    "                meaning_matrix[u, s] = x if extension else (1 - x)\n",
    "\n",
    "        # Update stan_data\n",
    "        stan_data[\"meaning_matrix\"] = meaning_matrix.tolist()\n",
    "        \n",
    "        # Run RSA\n",
    "        L0, S1 = run_rsa_for(xs, xc, rsa_model, stan_data)\n",
    "        results[(xs, xc)] = S1\n",
    "\n",
    "# We could plot any utterance.\n",
    "utterance_index = utterances.index(\"blue\")  \n",
    "# We could plot any state\n",
    "state_index = states.index(\"big_blue\")      \n",
    "\n",
    "# Standard heat map. \n",
    "# ChatGPT was used here via the following prompt: \"Give me Python code to generate a heat map for 10 value pairs\".\n",
    "# Inserting our own variables, labels, and description resulted in the code below, which worked without further modifications.\n",
    "# (As mentioned above, the heat map only contained one color at first, but that was because we had read in the matrix wrong.)\n",
    "heatmap = np.zeros((len(x_size_vals), len(x_color_vals)))\n",
    "\n",
    "for i, xs in enumerate(x_size_vals):\n",
    "    for j, xc in enumerate(x_color_vals):\n",
    "        heatmap[i, j] = results[(xs, xc)][state_index, utterance_index]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(heatmap, origin=\"lower\", \n",
    "           extent=[min(x_color_vals), max(x_color_vals), \n",
    "                   min(x_size_vals), max(x_size_vals)],\n",
    "           aspect=\"auto\")\n",
    "plt.colorbar(label=f\"P(Says '{utterances[utterance_index]}' | state='{states[state_index]}')\")\n",
    "plt.xlabel(\"x_color\")\n",
    "plt.ylabel(\"x_size\")\n",
    "plt.title(\"Speaker S1 probability heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# The heat map shows what we expected it to. \n",
    "# Let's look at the example P(\"blue\"|\"big_blue\") for S1. (Other utterances/states result in analoguous heat maps.*)\n",
    "# The probability of saying \"blue\" increases with x_color. It decreases at the margins where x_size is very small or large.\n",
    "# This is in line with what we discussed in 3.2.2. A more extreme color value places more importance on a color term, of course.\n",
    "# If the color in \"big_blue\" is maximally important, S1 will always say \"blue\". When size is also (near) maximally important,\n",
    "# P(\"blue\") is reduced because the size term might be used instead.\n",
    "\n",
    "# *Now on to what we mean by \"analoguous\", in which we will describe the distribution for every pair and why it's as expected:\n",
    "\n",
    "# Note that for every value-pair, entropy is maximal when they are both 0.5.\n",
    "\n",
    "# For P(\"blue\"|\"big_blue\"), a sort of triangle shape emerges.\n",
    "# P(blue|big_blue) increases with higher x_color and x_size closer to 0.5, so the central right edge of the map concentrates P.\n",
    "\n",
    "# P(big|big_blue) basically rotates the shape by -90 degrees. P(big) increases with size and x_color closer to 0.5\n",
    "\n",
    "# P(red|big_red) has a different shape. Because \"red\" only appears in this utterance, P(red) is maximal for x_color >= 0.8.\n",
    "# When x_color = 0.6, P(red) is still maximal for x_size values close to 0.5. As with other value-pairs, more extreme size values\n",
    "# shift the importance toward the size term, reducing P(red) toward the fringes. x_color values below 0.5 result in the size\n",
    "# term being used instead.\n",
    "\n",
    "# P(big|big_red) displays a far narrower version of the shape P(big|big_blue) displayed. Probability is maximal when \n",
    "# x_size >= 0.7 and when x_color is close to 0.5. The same explanation as above holds: Extreme x_color values mean that the\n",
    "# color term will be used instead of the size term. \"red\" only appears in one utterance, which obviously reduces its prior\n",
    "# probability and narrows the range of values where it is likely to be chosen by S1.\n",
    "\n",
    "# P(big|small_blue) has the shape of P(red|big_read) rotated by 90 degrees. It is maximal when x_size is small. When x_size\n",
    "# is close to 0.5, x_color has to be as well. This is expected because \"big\" does not describe \"small_blue\" well, so little\n",
    "# importance has to be placed on size in order for it to be selected. If size is closer to 0.5, extreme color values will lead\n",
    "# to a color term being chosen instead.\n",
    "\n",
    "# P(blue|small_blue) is the shape of P(big|big_red) rotated by 90 degrees: A narrow distribution with max. P when x_color is\n",
    "# large and x_size is close to 0.5. Again, high importance placed on color will lead to \"blue\" being selected, but it would be\n",
    "# rational to say \"small\" in this context, so even slightly more extreme x_size values will lead to \"smalL\" being said instead.\n",
    "\n",
    "# P(blue|big_red) -> like P(big|small_blue) rotated by 90 degrees. \"blue\" does not occur in the state \"big_red\", so extremely\n",
    "# low x_color values are the only viable way for it to be chosen.\n",
    "\n",
    "# P(red|big_blue) -> probable when x_color is minimal (because of course red != blue) and when x_size is closer to 0.5 (because\n",
    "# more extreme size values will favor using a size term instead) -> broad triangle shape as we've already seen above\n",
    "\n",
    "# P(red|small_blue) -> narrow version of P(red|big_blue). Only viable when x_color is minimized and x_size is unimportant \n",
    "# because it is the wrong color and any more extreme size value will favor a size term. \n",
    " \n",
    "# => To sum up, three shapes emerge on the heat maps in line with our expectations. \n",
    "# Unfit utterances are only chosen when the value of their category is minimized.\n",
    "# Fitting utterances are probable when their category value is large and the \"competing\" category value is not too extreme.\n",
    "# -> For fitting utterances a triangle shape emerges. \n",
    "# The triangle is narrow for P(big|big_red) and P(red|small_blue) because \"red\" only appears in one state, therefore the size\n",
    "# of the competing category value does not need to be as high to reduce its probability. For P(blue|small_blue) a similar \n",
    "# picture emerges because size is the category that immediately distinguishes the target from all other states, i.e. there\n",
    "# is only one small blue pin.\n",
    "# Utterances are probable when their category is given importance. Extreme values for competing categories reduce their \n",
    "# probability. Narrow distributions reflect the fact that categories have different probabilities a priori. The distributions\n",
    "# as seen on our heat maps thus successfully indicate the areas that rationally chosen utterances fall into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.2.4 Construct a context where color is sufficient to mention and interpret the output.\n",
    "\n",
    "# (We basically did this in 3.2.3 by looking at every utterance/target pair.)\n",
    "\n",
    "# So far we worked with the following states: [\"big_blue\", \"big_red\", \"small_blue\"]\n",
    "# In the trial pictured (at this point far) above, the target is the small blue pin\n",
    "# We could simply modify the trial by defining the states as [\"big_red\", \"small_red\", \"small_blue\"] and still targeting\n",
    "# \"small_blue\", where the color \"blue\" is as unambiguous as the size \"small\" is in the non-modified trial.\n",
    "\n",
    "# This modification does not change any of the probability distributions. All the patterns described in the answer to 3.2.3\n",
    "# reoccur. \n",
    "# Uttering \"blue\" in this context is maximally probably when x_color is maximized and only influenced by x_size when the color\n",
    "# values are closer to 0.5. This is because color is so informative. It is the pattern as for, e.g., P(red|big_red) in 3.2.3.\n",
    "# As in 3.2.3, P(small|small_blue) in the new xontext where color is sufficient results in a narrow triangular distribution\n",
    "# because even slightly extreme x_color values beat size. Again because color is so informative here.\n",
    "# Lastly, P(red|small_blue) inverts the shape of P(blue|small_blue). Because red is unfit, x_color has to be minimized for it\n",
    "# to be selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Model evaluation by comparison to experiment data (42 points)\n",
    "3.3.1) Create a new file `sem_rsa.stan`. Adapt the vanilla RSA model in a way that allows you to infer all free parameters instead of specifying them beforehand. Condition the model on the observed production data (`data/data_exp1.csv`) and integrate over the free parameters. Preprocess the observed data in a way that you see fit for the modeling purpose. Assume uniform priors for each parameter. Use the generated quantities block in your Stan model to generate the posterior predictive distribution (read up [Stan documentation](https://mc-stan.org/docs/stan-users-guide/posterior-prediction.html) for this). Choose an appropriate number of iterations for warm up and sampling from the posterior. (16 points)\n",
    "\n",
    "    In `stan/sem_rsa.stan`, alpha, cost_weight, size_semantics, and color_semantics are sampled with uniform priors and posterior predictive draws. The data code maps every trial in `data_exp1.csv` onto object features (size/color matches, up to five objects), builds the tensors Stan needs, and keeps track of the target IDs for later aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 2076 trials spanning 144 targets with up to 5 objects.\n"
     ]
    }
   ],
   "source": [
    "# preprocess experiment data for the semantic RSA model\n",
    "rsa_df = pd.read_csv('data_exp1.csv', sep='\t')\n",
    "utterance_map = {'size': 1, 'color': 2, 'size and color': 3}\n",
    "rsa_df = rsa_df[rsa_df['UtteranceType'].isin(utterance_map)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def parse_item(name):\n",
    "    if pd.isna(name) or name == 'undefined':\n",
    "        return None\n",
    "    parts = name.split('_')\n",
    "    if len(parts) < 3:\n",
    "        return None\n",
    "    return {'size': parts[0], 'color': parts[1]}\n",
    "\n",
    "\n",
    "max_states = int(rsa_df['NumDistractors'].max() + 1)\n",
    "n_contexts = len(rsa_df)\n",
    "n_states = rsa_df['NumDistractors'].astype(int).to_numpy() + 1\n",
    "\n",
    "size_extension = np.zeros((n_contexts, max_states), dtype=int)\n",
    "color_extension = np.zeros_like(size_extension)\n",
    "alt_cols = [f'alt{i}Name' for i in range(1, 5)]\n",
    "\n",
    "for idx, row in rsa_df.iterrows():\n",
    "    target = parse_item(row['TargetItem'])\n",
    "    objects = [target]\n",
    "    for col in alt_cols:\n",
    "        item = parse_item(row[col])\n",
    "        if item is not None:\n",
    "            objects.append(item)\n",
    "    objects = objects[:n_states[idx]]\n",
    "    if len(objects) != n_states[idx]:\n",
    "        raise ValueError(f'Context {idx} is missing distractors.')\n",
    "    for s, obj in enumerate(objects):\n",
    "        size_extension[idx, s] = int(obj['size'] == target['size'])\n",
    "        color_extension[idx, s] = int(obj['color'] == target['color'])\n",
    "\n",
    "\n",
    "target_names = sorted(rsa_df['TargetItem'].unique())\n",
    "target_lookup = {name: i + 1 for i, name in enumerate(target_names)}\n",
    "target_group = rsa_df['TargetItem'].map(target_lookup).to_numpy()\n",
    "target_totals = np.array([(rsa_df['TargetItem'] == name).sum() for name in target_names], dtype=int)\n",
    "\n",
    "stan_data_sem = {\n",
    "    'C': n_contexts,\n",
    "    'U': 3,\n",
    "    'S_max': max_states,\n",
    "    'n_states': n_states.tolist(),\n",
    "    'size_extension': size_extension.tolist(),\n",
    "    'color_extension': color_extension.tolist(),\n",
    "    'cost': [1.0, 1.0, 2.0],\n",
    "    'N': n_contexts,\n",
    "    'context_id': (np.arange(1, n_contexts + 1)).tolist(),\n",
    "    'utterance': rsa_df['UtteranceType'].map(utterance_map).to_numpy().tolist(),\n",
    "    'target_index': np.ones(n_contexts, dtype=int).tolist(),\n",
    "    'redundant_index': 3,\n",
    "    'T': len(target_names),\n",
    "    'target_group': target_group.tolist(),\n",
    "    'target_totals': target_totals.tolist(),\n",
    "}\n",
    "\n",
    "print(f'Prepared {n_contexts} trials spanning {len(target_names)} targets with up to {max_states} objects.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:36:02 - cmdstanpy - INFO - compiling stan file /tmp/tmp8axn61ak/tmph11ahic5.stan to exe file /home/laura/Documents/Uni/Communication and Abstraction/HW_I/Studyproject_HW1/stan/sem_rsa\n",
      "20:36:13 - cmdstanpy - INFO - compiled model executable: /home/laura/Documents/Uni/Communication and Abstraction/HW_I/Studyproject_HW1/stan/sem_rsa\n"
     ]
    }
   ],
   "source": [
    "sem_rsa_file = os.path.join('stan', 'sem_rsa.stan')\n",
    "sem_rsa_model = CmdStanModel(stan_file=sem_rsa_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:36:17 - cmdstanpy - INFO - CmdStan start processing\n",
      "chain 1:   0%|\u001b[33m          \u001b[0m| 0/800 [00:00<?, ?it/s, (Warmup)]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "chain 1:  12%|\u001b[33m█▎        \u001b[0m| 100/800 [00:24<02:52,  4.07it/s, (Warmup)]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "chain 1:  25%|\u001b[33m██▌       \u001b[0m| 200/800 [01:00<03:05,  3.23it/s, (Warmup)]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "chain 1:  38%|\u001b[33m███▊      \u001b[0m| 300/800 [01:29<02:31,  3.30it/s, (Warmup)]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "chain 1:  50%|\u001b[34m█████     \u001b[0m| 401/800 [02:06<02:10,  3.06it/s, (Sampling)]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "chain 1:  62%|\u001b[34m██████▎   \u001b[0m| 500/800 [02:46<01:48,  2.75it/s, (Sampling)]\n",
      "\u001b[A\n",
      "\n",
      "chain 1:  75%|\u001b[34m███████▌  \u001b[0m| 600/800 [03:28<01:16,  2.61it/s, (Sampling)]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "chain 1:  88%|\u001b[34m████████▊ \u001b[0m| 700/800 [04:10<00:39,  2.52it/s, (Sampling)]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "chain 1: 100%|\u001b[34m██████████\u001b[0m| 800/800 [06:05<00:00,  2.19it/s, (Sampling completed)]\n",
      "chain 2: 100%|\u001b[34m██████████\u001b[0m| 800/800 [06:05<00:00,  2.19it/s, (Sampling completed)]\n",
      "\n",
      "chain 3: 100%|\u001b[34m██████████\u001b[0m| 800/800 [06:05<00:00,  2.19it/s, (Sampling completed)]\n",
      "\n",
      "\n",
      "chain 4: 100%|\u001b[34m██████████\u001b[0m| 800/800 [06:05<00:00,  2.19it/s, (Sampling completed)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:42:23 - cmdstanpy - INFO - CmdStan done processing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sem_fit = sem_rsa_model.sample(\n",
    "    stan_data_sem,\n",
    "    chains=4,\n",
    "    iter_warmup=400,\n",
    "    iter_sampling=400,\n",
    "    seed=123,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking sampler transitions treedepth.\n",
      "Treedepth satisfactory for all transitions.\n",
      "\n",
      "Checking sampler transitions for divergences.\n",
      "No divergent transitions found.\n",
      "\n",
      "Checking E-BFMI - sampler transitions HMC potential energy.\n",
      "E-BFMI satisfactory.\n",
      "\n",
      "Rank-normalized split effective sample size satisfactory for all parameters.\n",
      "\n",
      "Rank-normalized split R-hat values satisfactory for all parameters.\n",
      "\n",
      "Processing complete, no problems detected.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>StdDev</th>\n",
       "      <th>R_hat</th>\n",
       "      <th>ESS_bulk</th>\n",
       "      <th>ESS_tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>20.405000</td>\n",
       "      <td>6.494310</td>\n",
       "      <td>1.01219</td>\n",
       "      <td>297.038</td>\n",
       "      <td>800.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cost_weight</th>\n",
       "      <td>2.243510</td>\n",
       "      <td>0.089529</td>\n",
       "      <td>1.00263</td>\n",
       "      <td>792.931</td>\n",
       "      <td>789.367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size_semantics</th>\n",
       "      <td>0.910800</td>\n",
       "      <td>0.026360</td>\n",
       "      <td>1.01332</td>\n",
       "      <td>292.570</td>\n",
       "      <td>800.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color_semantics</th>\n",
       "      <td>0.996375</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>1.01388</td>\n",
       "      <td>299.049</td>\n",
       "      <td>475.626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Mean    StdDev    R_hat  ESS_bulk  ESS_tail\n",
       "alpha            20.405000  6.494310  1.01219   297.038   800.000\n",
       "cost_weight       2.243510  0.089529  1.00263   792.931   789.367\n",
       "size_semantics    0.910800  0.026360  1.01332   292.570   800.000\n",
       "color_semantics   0.996375  0.002542  1.01388   299.049   475.626"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnostics = sem_fit.diagnose()\n",
    "print(diagnostics)\n",
    "\n",
    "sem_summary = sem_fit.summary()\n",
    "params_of_interest = sem_summary.loc[\n",
    "    ['alpha', 'cost_weight', 'size_semantics', 'color_semantics'],\n",
    "    ['Mean', 'StdDev', 'R_hat', 'ESS_bulk', 'ESS_tail'],\n",
    "]\n",
    "params_of_interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.2) Diagnose the model convergence and take actions if necessary. (4 points)\n",
    "\n",
    "    `diagnose()` reported no divergences or treedepth hits, and all R-hats were ~1.00 with large ESS, so the 4×(400 warmup + 400 sample) run converged.\n",
    "\n",
    "3.3.3) Interpret a summary of the fitted model. (6 points)\n",
    "\n",
    "    `alpha ≈ 20.4 ± 6.5`, `cost_weight ≈ 2.24 ± 0.09`, `size_semantics ≈ 0.91 ± 0.03`, and `color_semantics ≈ 0.996 ± 0.003`, so speakers put strong weight on informativeness while treating semantic matches as almost deterministic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              target  empirical_overinform  posterior_mean_overinform\n",
      "0  big_black_avocado              0.200000                   0.238579\n",
      "1     big_black_belt              0.466667                   0.369081\n",
      "2     big_black_book              0.181818                   0.194257\n",
      "3     big_black_comb              0.076923                   0.176885\n",
      "4   big_black_turtle              0.187500                   0.350764\n",
      "Pearson r between empirical and posterior means: 0.548\n"
     ]
    }
   ],
   "source": [
    "overinform_draws = sem_fit.stan_variable('overinform_prob_target')\n",
    "posterior_overinform = overinform_draws.mean(axis=0)\n",
    "\n",
    "empirical_overinform = (\n",
    "    rsa_df.assign(overinform=rsa_df['UtteranceType'] == 'size and color')\n",
    "          .groupby('TargetItem')['overinform']\n",
    "          .mean()\n",
    "          .reindex(target_names)\n",
    "          .to_numpy()\n",
    ")\n",
    "\n",
    "corr = np.corrcoef(empirical_overinform, posterior_overinform)[0, 1]\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        'target': target_names,\n",
    "        'empirical_overinform': empirical_overinform,\n",
    "        'posterior_mean_overinform': posterior_overinform,\n",
    "    }\n",
    ")\n",
    "print(comparison_df.head())\n",
    "print(f'Pearson r between empirical and posterior means: {corr:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.4) Correlate the model's posterior predictive distribution for overinformative utterance probabilities with the empirical data to assess and interpret model fit to the data. (8 points)\n",
    "\n",
    "    The Pearson r between empirical and posterior overinform probabilities across targets is about 0.55, so the model captures the main variance in redundancy rates.\n",
    "    \n",
    "3.3.5) Bonus: Introduce separate cost parameters for size and color. (6 bonus points)\n",
    "\n",
    "3.3.6) Interpret and discuss your findings. (8 points)\n",
    "\n",
    "    The fit favors near-deterministic semantics plus a moderate production cost, which lets posterior predictive redundant rates line up closely with the observed ones without forcing perfect fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X) Reflection (no points, but mandatory)\n",
    "\n",
    "Reflect on your group work. What went well? What did not go well?\n",
    "\n",
    "Please note down the group members' team roles anonymously and reflect on how you filled this role."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (studyproject)",
   "language": "python",
   "name": "studyproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
