{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:13:57.889461Z",
     "start_time": "2025-11-10T13:13:57.886996Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laura/miniconda3/envs/abstraction/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from cmdstanpy import CmdStanModel\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Bayesian Cognitive and Rational Speech Act models\n",
    "\n",
    "This homework assignment is to be completed in groups. It is due on November 27, 2025 (midnight). Please upload *all files you created or modified* to the homework folder of your group in studIP.\n",
    "\n",
    "Group number:\n",
    "\n",
    "Names:\n",
    "\n",
    "*General note: It is permitted to use AI tools for coding. Please refer to the uploaded manual `AI_Tools_Guidelines` for recommended ways how to use AI to advance your studies in a way that supports your learning. That means that you should not be satisfied if an AI tool hands you a working version of your code, but that you should put in effort to understand how exactly the problem is solved. Another note of caution: What might work for large programming languages like Python, does not necessarily work for Stan. Check your code carefully and do NOT blindly trust AI.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "During the past weeks, you have learned how Bayesian inference works and how it can be used in Bayesian cognitive models. You also learned about a specific type of Bayesian models that can be used to model pragmatic language understanding and production, the Rational Speech Act models. The goal of this homework assignment is for you to learn how to implement Bayesian models in Stan and, specifically, how to implement RSA models in Stan. A special focus will be on the different use cases and evaluation methods of RSA models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Stan modeling (8 points)\n",
    "\n",
    "1.1) In the file `simple_model.stan`, you will find a simple Stan model. Describe its implementation, relating it to the knowledge you gained about the conventions for coding models in Stan. (4 points)\n",
    "\n",
    "- The data block defines the observed data: we have N trials, and for each trial we know the semantic similarity, the phonological similarity, and the response time.\n",
    "- In the transformed data block, the response times are log-transformed, so the model works on log_rt. This makes the normality assumption more reasonable, because raw response times are positive and often skewed.\n",
    "- In the parameters block, the model estimates:\n",
    "    - alpha (intercept),\n",
    "    - beta_sem (effect of semantic similarity),\n",
    "    - beta_phon (effect of phonological similarity),\n",
    "    - sigma_rt (residual standard deviation, constrained to be positive).\n",
    "    - Each of these parameters has a weakly informative normal prior.\n",
    "- In the model block, each log_rt[i] is assumed to follow a normal distribution with mean alpha + beta_sem * semantic_similarity[i] + beta_phon * phonological_similarity[i] and standard deviation sigma_rt. So the two similarity measures linearly predict log response time.\n",
    "- In the generated quantities block:\n",
    "    - log_likelihood[i] stores the pointwise log density for each trial,\n",
    "    - log_rt_ppd[i] stores posterior-predictive draws on the log scale,\n",
    "    - rt_ppd[i] exponentiates those draws back to the original time scale, giving predicted response times for posterior predictive checks.\n",
    "\n",
    "Essentially, it’s a Bayesian linear regression model that predicts how fast someone responds based on how semantically and phonologically related the word and context are.\n",
    "\n",
    "1.2) You will notice that the model does not compile. Fix the problems and explain what you did. (4 points)\n",
    "\n",
    "    The model compiles:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Bayesian cognitive models (10 points)\n",
    "Think of a use case for `simple_model.stan` in the scope of Bayesian cognitive modeling. Describe the model while answering the following questions:\n",
    "\n",
    "2.1) What cognitive capacity can be explained by this model? (2 points)\n",
    "\n",
    "- The model describes lexical access in tasks like visual lexical decision or picture naming: how quickly a person can retrieve the correct word form when given semantic and phonological cues from a context.\n",
    "- The predictors capture two psychologically meaningful influences:\n",
    "    - semantic priming: effect of meaning similarity,\n",
    "    - phonological priming: effect of sound similarity,\n",
    "- and the strength of these influences changes the observed response times.\n",
    "\n",
    "2.2) What is the purpose and function of this model? (3 points)\n",
    "\n",
    "- The model explains how semantic and phonological similarity jointly produce the distribution of RTs we observe.\n",
    "- By fitting this Bayesian regression, we can:\n",
    "    - estimate how much semantic and phonological similarity speed up or slow down retrieval,\n",
    "    - quantify priming effects at the level of participants or conditions,\n",
    "    - generate posterior predictive RT distributions for new items or contexts,\n",
    "    - compare hypotheses (e.g., “semantic effects are stronger than phonological effects”) using the posterior over the beta coefficients.\n",
    "\n",
    "2.3) At which level of analysis does it model this cognitive capacity and why? (3 points)\n",
    "\n",
    "- The Stan model is a computational model in Marr’s framework. It specifies what mapping the cognitive system implements: given inputs (semantic similarity and phonological similarity), it defines a probability distribution over response times.\n",
    "- The code does not specify an algorithm or representation format for lexical access (no activation dynamics, no specific retrieval steps, no architecture), so it is not an algorithmic-level model. And it does not mention neural structures or hardware, so it is not an implementational-level model either. It remains at the abstract, probabilistic computational-level description of the capacity “map semantic/phonological similarity to response-time distributions.”\n",
    "\n",
    "\n",
    "Overall coherence gives another 2 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) RSA modeling (82 points)\n",
    "The purpose of the following model is to explain the use of overinformative referring expressions in pragmatic communication. In referential communication, the speaker’s task is to produce a referring expression that allows a listener to identify the target in the context. Consider the context below, where the target is the small blue pin. A referring expression including a size adjective (the small pin) is strictly speaking sufficient for uniquely establishing reference to the target, yet speakers often “overmodify” with color, producing referring expressions like the small blue pin. This overmodification phenomenon is what the model is intended to capture.\n",
    "\n",
    "<img src=\"img/size-sufficient.png\" width=\"400\"/>\n",
    "\n",
    "### 3.1) Vanilla RSA (20 points)\n",
    "In the file `vanilla_rsa.stan`, you find an RSA model of the production of referring expressions, based on the vanilla RSA model of Frank & Goodman (2012) that we discussed in class.\n",
    "\n",
    "3.1.1) Provide informative comments in the file `vanilla_rsa.stan`. (4 points)\n",
    "\n",
    "3.1.2) You will notice that the parameters and model blocks are empty. Why is that? Go through the following code and inspect the model's behavior. Look at the stan variables that are included in the fitted model. (3 points)\n",
    "\n",
    "    There are no parameters and no defined model, as we are not doing Bayesian sampling. We are not trying to estimate any values but rather arrive at our results deterministically by doing matrix transformations within transformed parameters{}.\n",
    "\n",
    "    We pass our data to stan as a dictionary. It is important to note that stan does not accept matrices and arrays, so these have to be transformed into lists first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T09:58:12.006198Z",
     "start_time": "2025-11-11T09:58:11.938342Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:48:15 - cmdstanpy - INFO - compiling stan file C:\\Users\\User\\Proton Drive\\maa_ng0h\\My files\\C. Work\\2E Modeling Communication and Abstraction\\Studyproject_HW1\\stan\\vanilla_rsa.stan to exe file C:\\Users\\User\\Proton Drive\\maa_ng0h\\My files\\C. Work\\2E Modeling Communication and Abstraction\\Studyproject_HW1\\stan\\vanilla_rsa.exe\n",
      "23:48:36 - cmdstanpy - INFO - compiled model executable: C:\\Users\\User\\Proton Drive\\maa_ng0h\\My files\\C. Work\\2E Modeling Communication and Abstraction\\Studyproject_HW1\\stan\\vanilla_rsa.exe\n",
      "23:48:36 - cmdstanpy - INFO - compiled model executable: C:\\Users\\User\\Proton Drive\\maa_ng0h\\My files\\C. Work\\2E Modeling Communication and Abstraction\\Studyproject_HW1\\stan\\vanilla_rsa.exe\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "stan_file = os.path.join('stan', 'vanilla_rsa.stan')\n",
    "rsa_model = CmdStanModel(stan_file=stan_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T13:16:47.445896Z",
     "start_time": "2025-11-10T13:16:47.442173Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:48:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "23:48:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "23:48:51 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain [1] method = sample (Default)\n",
      "Chain [1] sample\n",
      "Chain [1] num_samples = 300\n",
      "Chain [1] num_warmup = 300\n",
      "Chain [1] save_warmup = 0 (Default)\n",
      "Chain [1] thin = 1 (Default)\n",
      "Chain [1] adapt\n",
      "Chain [1] engaged = 1 (Default)\n",
      "Chain [1] gamma = 0.050000000000000003 (Default)\n",
      "Chain [1] delta = 0.80000000000000004 (Default)\n",
      "Chain [1] kappa = 0.75 (Default)\n",
      "Chain [1] t0 = 10 (Default)\n",
      "Chain [1] init_buffer = 75 (Default)\n",
      "Chain [1] term_buffer = 50 (Default)\n",
      "Chain [1] window = 25 (Default)\n",
      "Chain [1] algorithm = hmc (Default)\n",
      "Chain [1] hmc\n",
      "Chain [1] engine = nuts (Default)\n",
      "Chain [1] nuts\n",
      "Chain [1] max_depth = 10 (Default)\n",
      "Chain [1] metric = diag_e (Default)\n",
      "Chain [1] metric_file =  (Default)\n",
      "Chain [1] stepsize = 1 (Default)\n",
      "Chain [1] stepsize_jitter = 0 (Default)\n",
      "Chain [1] num_chains = 1 (Default)\n",
      "Chain [1] id = 1 (Default)\n",
      "Chain [1] data\n",
      "Chain [1] file = C:\\Users\\User\\AppData\\Local\\Temp\\tmpg4mr39y9\\5fo9opp4.json\n",
      "Chain [1] init = 2 (Default)\n",
      "Chain [1] random\n",
      "Chain [1] seed = 98410\n",
      "Chain [1] output\n",
      "Chain [1] file = C:\\Users\\User\\AppData\\Local\\Temp\\tmpg4mr39y9\\vanilla_rsalga7yqju\\vanilla_rsa-20251127234851.csv\n",
      "Chain [1] diagnostic_file =  (Default)\n",
      "Chain [1] refresh = 100 (Default)\n",
      "Chain [1] sig_figs = -1 (Default)\n",
      "Chain [1] profile_file = profile.csv (Default)\n",
      "Chain [1] num_threads = 1 (Default)\n",
      "Chain [1] \n",
      "Chain [1] Model contains no parameters, running fixed_param sampler, no updates to Markov chain\n",
      "Chain [1] Iteration:   1 / 300 [  0%]  (Sampling)\n",
      "Chain [1] Iteration: 100 / 300 [ 33%]  (Sampling)\n",
      "Chain [1] Iteration: 200 / 300 [ 66%]  (Sampling)\n",
      "Chain [1] Iteration: 300 / 300 [100%]  (Sampling)\n",
      "Chain [1] \n",
      "Chain [1] Elapsed Time: 0 seconds (Warm-up)\n",
      "Chain [1] 0.014 seconds (Sampling)\n",
      "Chain [1] 0.014 seconds (Total)\n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n"
     ]
    }
   ],
   "source": [
    "# define input data\n",
    "states = [\"big_blue\", \"big_red\", \"small_blue\"]\n",
    "utterances = [\n",
    "    \"big\", \"small\", \"blue\", \"red\"\n",
    "]\n",
    "\n",
    "n_states = len(states)\n",
    "n_utterances   = len(utterances)\n",
    "\n",
    "# build meaning_matrix[u, s]\n",
    "meaning_matrix = np.zeros((n_utterances, n_states), dtype=int)\n",
    "for u, utterance in enumerate(utterances):\n",
    "    for s, state in enumerate(states):\n",
    "        # literal meaning maps to true iff the utterance string appears in the state string\n",
    "        # Stan cannot handle booleans, so we need to work with integers here\n",
    "        meaning_matrix[u, s] = int(utterance in state)\n",
    "\n",
    "# parameters - change them here\n",
    "alpha = 1.0\n",
    "cost_weight = 1.0\n",
    "\n",
    "# cost function\n",
    "cost_dict = {\n",
    "    \"big\": 0.0,\n",
    "    \"small\": 0.0,\n",
    "    \"blue\": 0.0,\n",
    "    \"red\": 0.0,\n",
    "}\n",
    "cost = np.array([cost_dict[utterance] for utterance in short_utterances])\n",
    "\n",
    "\n",
    "# prepare Stan data as dictionary\n",
    "stan_data = {\n",
    "    \"S\": n_states,\n",
    "    \"U\": n_utterances,\n",
    "    \"meaning_matrix\": meaning_matrix.tolist(), # Stan cannot handle numpy arrays       \n",
    "    \"cost\": cost.tolist(),                     # or dictionaries\n",
    "    \"alpha\": alpha,\n",
    "    \"cost_weight\": cost_weight\n",
    "}\n",
    "\n",
    "fit = rsa_model.sample(stan_data, show_console=True, chains=1, iter_warmup=300, adapt_engaged=True, iter_sampling=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0:\n",
      "       big_blue  big_red  small_blue\n",
      "big        0.50     0.50        0.00\n",
      "small      0.00     0.00        1.00\n",
      "blue       0.50     0.00        0.50\n",
      "red        0.00     1.00        0.00\n",
      "S1:\n",
      "            big  small  blue  red\n",
      "big_blue   0.50   0.00  0.50 0.00\n",
      "big_red    0.33   0.00  0.00 0.67\n",
      "small_blue 0.00   0.67  0.33 0.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "L0_draws = fit.stan_variable('L0')\n",
    "S1_draws = fit.stan_variable('S1')\n",
    "\n",
    "L0 = L0_draws[0]\n",
    "S1 = S1_draws[0]\n",
    "\n",
    "df_L0 = pd.DataFrame(L0, index=utterances, columns=states)\n",
    "df_S1 = pd.DataFrame(S1, index=states, columns=utterances)\n",
    "\n",
    "print(\"L0:\")\n",
    "print(df_L0.to_string(float_format=\"{:.2f}\".format))\n",
    "print(\"S1:\")\n",
    "print(df_S1.to_string(float_format=\"{:.2f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.3) Are the outputs in line with what you would expect given your knowledge about pragmatic communication and overinformative referring expressions?\n",
    "Add complex utterances to the model (i.e., utterance consisting of a size and color adjective) and inspect the output again. The meaning of a complex two-word utterance is defined with intuitive intersective semantics: $$\\mathcal{L}(u_{\\text{complex}}, o)=\\mathcal{L}(u_{\\text{size}},o)\\times\\mathcal{L}(u_{\\text{color}},o)$$ (6 points)\n",
    "\n",
    "    I think the outputs are matching my intitions about pragmatic communication. We can see that the utterance \"red\" has a higher average utility value than \"blue\". This is expected, because the there is only one state (\"big_red\") that can correspond with \"red\" while blue could refer to two states (\"big_blue\", \"small_blue\").\n",
    "    \n",
    "    When adding a complex utterance where its consituence are found in the states but not in that combination (e.g. small_red), it's corresponding row in L0 contains NaN and consequentially the entire S1 matrix is filled with NaN. \n",
    "    \n",
    "    If we add the three complex utterances that fully describe the states, we can observe that in S1, the utility of complex utterances is higher than the non-complex utterances, as long as there is no production cost that punishes utterance length. This makes intuitive sense, as complex utterances are more informatively describing the state than non-complex ones.\n",
    "\n",
    "3.1.4) Play around with the rationality and cost weight parameters. How do they affect the model output? (4 points)\n",
    "\n",
    "    Negative Values are prohibited, it throws an error.\n",
    "    The lower rationality, the more uniform are the values distributed. \n",
    "    The higher the costs, the more non-complex statements are valued.\n",
    "    Further, I created dynamic cost functions, that determine uttarance cost based on word length or letter length. With a positive letter length cost, the utterance \"red\" has lower cost than the utterance \"blue\". \n",
    "\n",
    "3.1.5) Adapt the utterance cost in a way that achieves a preference for overinformative referring expressions. (2 points)\n",
    "\n",
    "    Setting Cost weight to 0 is one way to increase overinformativeness. Another is making the \"per_letter\" or \"per_underscore\" values in the custom cost functions negative, so that short expressions are effectively punished.\n",
    "\n",
    "3.1.6) Adapt the utterance cost in a way that seems most natural to you. (1 point)\n",
    "\n",
    "    I decided to use the custom \"cost_per_underscore\" function, as it dynamically calculates costs for each utterance based on word count. It is both pragmatic and natural, compared to the second custom cost function that weights based on letter count.. Further, I chose moderate cost_weight and per_underscore, because I the cost of typing or understanding another word in this situation is not very high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "397bccbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:49:19 - cmdstanpy - INFO - CmdStan start processing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce7696ab32a48eaaf18e0c0a4657641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chain 1 |          | 00:00 Status"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:49:19 - cmdstanpy - INFO - CmdStan done processing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# define input data\n",
    "states = [\"big_blue\", \"big_red\", \"small_blue\"]\n",
    "\n",
    "utterances = [\n",
    "    \"big\", \"small\", \"blue\", \"red\", \"big_red\", \"big_blue\", \"small_blue\"\n",
    "]\n",
    "\n",
    "n_states = len(states)\n",
    "n_utterances   = len(utterances)\n",
    "\n",
    "# build meaning_matrix[u, s]\n",
    "meaning_matrix = np.zeros((n_utterances, n_states), dtype=int)\n",
    "for u, utterance in enumerate(utterances):\n",
    "    for s, state in enumerate(states):\n",
    "        # literal meaning maps to true iff the utterance string appears in the state string\n",
    "        # Stan cannot handle booleans, so we need to work with integers here\n",
    "        meaning_matrix[u, s] = int(utterance in state)\n",
    "\n",
    "# parameters - change them here\n",
    "alpha = 0.8\n",
    "cost_weight = 0.3\n",
    "\n",
    "# new cost functions\n",
    "def cost_by_letters(utterance, per_letter=0.1):\n",
    "    letters = sum(1 for ch in utterance if ch.isalpha())\n",
    "    return letters * per_letter\n",
    "\n",
    "def cost_by_underscores(utterance, per_underscore=1.0):\n",
    "    return utterance.count('_') * per_underscore\n",
    "\n",
    "cost = np.array([cost_by_underscores(utterance) for utterance in utterances])\n",
    "\n",
    "# prepare Stan data as dictionary\n",
    "stan_data = {\n",
    "    \"S\": n_states,\n",
    "    \"U\": n_utterances,\n",
    "    \"meaning_matrix\": meaning_matrix.tolist(), # Stan cannot handle numpy arrays       \n",
    "    \"cost\": cost.tolist(),                     # or dictionaries\n",
    "    \"alpha\": alpha,\n",
    "    \"cost_weight\": cost_weight\n",
    "}\n",
    "\n",
    "fit = rsa_model.sample(stan_data, show_console=False, chains=1, iter_warmup=300, adapt_engaged=True, iter_sampling=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0191ef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0:\n",
      "            big_blue  big_red  small_blue\n",
      "big             0.50     0.50        0.00\n",
      "small           0.00     0.00        1.00\n",
      "blue            0.50     0.00        0.50\n",
      "red             0.00     1.00        0.00\n",
      "big_red         0.00     1.00        0.00\n",
      "big_blue        1.00     0.00        0.00\n",
      "small_blue      0.00     0.00        1.00\n",
      "S1:\n",
      "            big  small  blue  red  big_red  big_blue  small_blue\n",
      "big_blue   0.30   0.00  0.30 0.00     0.00      0.39        0.00\n",
      "big_red    0.25   0.00  0.00 0.43     0.32      0.00        0.00\n",
      "small_blue 0.00   0.43  0.25 0.00     0.00      0.00        0.32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "L0_draws = fit.stan_variable('L0')\n",
    "S1_draws = fit.stan_variable('S1')\n",
    "\n",
    "L0 = L0_draws[0]\n",
    "S1 = S1_draws[0]\n",
    "\n",
    "df_L0 = pd.DataFrame(L0, index=utterances, columns=states)\n",
    "df_S1 = pd.DataFrame(S1, index=states, columns=utterances)\n",
    "\n",
    "print(\"L0:\")\n",
    "print(df_L0.to_string(float_format=\"{:.2f}\".format))\n",
    "print(\"S1:\")\n",
    "print(df_S1.to_string(float_format=\"{:.2f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Relaxed semantics (20 points)\n",
    "It seems that our intuitions do not align well with the model. Let's use continuous rather than boolean semantics to see whether this can solve our problem. In the following, you need to adapt the RSA model and input data in a way that implements continuous semantics. The only change will be that the lexicon, or meaning matrix, should return real values instead of true or false: $$\\mathcal{L}(u,o)\\in [0,1] \\subset \\mathbb{R}$$\n",
    "This approach captures the intuition that an object is not unambiguously big or blue, but rather that objects can count as big or blue to varying degrees.\n",
    "\n",
    "3.2.1) Build a meaning matrix that captures the relaxed semantics with two new parameters size_semantics $x_\\text{size}$ and color_semantics $x_\\text{color}$. When an object $o$ is in the extension of a size adjective under the Boolean semantics defined above, take $\\mathcal{L}(u,o)=x_\\text{size}$, else $\\mathcal{L}(u,o)=1-x_\\text{size}$. The semantics are defined analogously for color. (6 points)\n",
    "3.2.2) Run the model with alpha = 30, size_semantics = 0.8 and color_semantics = 0.99. Inspect the model outputs. (4 points)\n",
    "3.2.3) Visualize the results of varying values for size_semantics and color_semantics, pit them against each other and interpret them. (6 points)\n",
    "3.2.4) Van Gompel et al. (2019) found that speakers use overinformative referring expressions in about 80% of the trials that look like the one above, where size is sufficient to mention. What about contexts where color is sufficient to mention? Construct a context where color is sufficient to mention and interpret the output. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "stan_file = os.path.join('stan', 'vanilla_rsa_relaxed_semantics.stan')\n",
    "rsa_model = CmdStanModel(stan_file=stan_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code for exercise 3.2, adapted from the code for exercise 3.1.\n",
    "\n",
    "# new parameters\n",
    "x_size = 0.8 \n",
    "x_color = 0.99\n",
    "\n",
    "# define input data\n",
    "states = [\"big_blue\", \"big_red\", \"small_blue\"]\n",
    "utterances = [\n",
    "    \"big\", \"small\", \"blue\", \"red\"\n",
    "]\n",
    "n_states = len(states)\n",
    "n_utterances   = len(utterances)\n",
    "\n",
    "# introduce size/color categorization\n",
    "size_utterances  = [\"big\", \"small\"]\n",
    "color_utterances = [\"blue\", \"red\"]\n",
    "\n",
    "# build meaning_matrix[u, s]\n",
    "meaning_matrix = np.zeros((n_utterances, n_states))\n",
    "for u, utterance in enumerate(utterances):\n",
    "    for s, state in enumerate(states):\n",
    "        # check if object is in the extension of an adjective under Boolean semantics\n",
    "        extension = utterance in state\n",
    "        # check utterance category\n",
    "        if utterance in size_utterances:\n",
    "            x = x_size\n",
    "        elif utterance in color_utterances:\n",
    "            x = x_color\n",
    "        # if object is in the extension of an adjective, take L(u,o) = x_size/color, else L(u,o) = 1 - x_size/color\n",
    "        # e.g. \"big\" refers to big objects with probabiliy x and small objects with probability 1-x\n",
    "        meaning_matrix[u, s] = x if extension else (1 - x)\n",
    "\n",
    "# parameters - change them here\n",
    "alpha = 30\n",
    "cost_weight = 1.0\n",
    "\n",
    "# cost function\n",
    "cost_dict = {\n",
    "    \"big\": 0.0,\n",
    "    \"small\": 0.0,\n",
    "    \"blue\": 0.0,\n",
    "    \"red\": 0.0,\n",
    "}\n",
    "cost = np.array([cost_dict[utterance] for utterance in utterances])\n",
    "\n",
    "# prepare Stan data as dictionary\n",
    "stan_data = {\n",
    "    \"S\": n_states,\n",
    "    \"U\": n_utterances,\n",
    "    \"meaning_matrix\": meaning_matrix.tolist(), # Stan cannot handle numpy arrays       \n",
    "    \"cost\": cost.tolist(),                     # or dictionaries\n",
    "    \"alpha\": alpha,\n",
    "    \"cost_weight\": cost_weight,\n",
    "    \"x_size\": x_size,\n",
    "    \"x_color\": x_color\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:52:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:52:02 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain [1] method = sample (Default)\n",
      "Chain [1] sample\n",
      "Chain [1] num_samples = 200\n",
      "Chain [1] num_warmup = 200\n",
      "Chain [1] save_warmup = 0 (Default)\n",
      "Chain [1] thin = 1 (Default)\n",
      "Chain [1] adapt\n",
      "Chain [1] engaged = 1 (Default)\n",
      "Chain [1] gamma = 0.050000000000000003 (Default)\n",
      "Chain [1] delta = 0.80000000000000004 (Default)\n",
      "Chain [1] kappa = 0.75 (Default)\n",
      "Chain [1] t0 = 10 (Default)\n",
      "Chain [1] init_buffer = 75 (Default)\n",
      "Chain [1] term_buffer = 50 (Default)\n",
      "Chain [1] window = 25 (Default)\n",
      "Chain [1] algorithm = hmc (Default)\n",
      "Chain [1] hmc\n",
      "Chain [1] engine = nuts (Default)\n",
      "Chain [1] nuts\n",
      "Chain [1] max_depth = 10 (Default)\n",
      "Chain [1] metric = diag_e (Default)\n",
      "Chain [1] metric_file =  (Default)\n",
      "Chain [1] stepsize = 1 (Default)\n",
      "Chain [1] stepsize_jitter = 0 (Default)\n",
      "Chain [1] num_chains = 1 (Default)\n",
      "Chain [1] id = 1 (Default)\n",
      "Chain [1] data\n",
      "Chain [1] file = C:\\Users\\Hannes\\AppData\\Local\\Temp\\tmpfz_r_e0l\\ck6t1zzj.json\n",
      "Chain [1] init = 2 (Default)\n",
      "Chain [1] random\n",
      "Chain [1] seed = 73760\n",
      "Chain [1] output\n",
      "Chain [1] file = C:\\Users\\Hannes\\AppData\\Local\\Temp\\tmpfz_r_e0l\\vanilla_rsa_relaxed_semanticsl_q_gxfd\\vanilla_rsa_relaxed_semantics-20251126185202.csv\n",
      "Chain [1] diagnostic_file =  (Default)\n",
      "Chain [1] refresh = 100 (Default)\n",
      "Chain [1] sig_figs = -1 (Default)\n",
      "Chain [1] profile_file = profile.csv (Default)\n",
      "Chain [1] num_threads = 1 (Default)\n",
      "Chain [1] \n",
      "Chain [1] Model contains no parameters, running fixed_param sampler, no updates to Markov chain\n",
      "Chain [1] Iteration:   1 / 200 [  0%]  (Sampling)\n",
      "Chain [1] Iteration: 100 / 200 [ 50%]  (Sampling)\n",
      "Chain [1] Iteration: 200 / 200 [100%]  (Sampling)\n",
      "Chain [1] \n",
      "Chain [1] Elapsed Time: 0 seconds (Warm-up)\n",
      "Chain [1] 0.01 seconds (Sampling)\n",
      "Chain [1] 0.01 seconds (Total)\n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n",
      "Chain [1] \n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.2.2: Run the model with alpha = 30, size_semantics = 0.8 and color_semantics = 0.99. Inspect the model outputs.\n",
    "\n",
    "# We observe that color behaves like in non-relaxed semantics. For L0, P(\"big_red\"|\"red\") = 1.0, reflecting that the color word\n",
    "# practically determines that a pin of its color is chosen. We say \"practically\" because 0.99 is of course not 100%, but in\n",
    "# practice we get a 1.0 probability. Again for L0, P(\"big_blue\"|\"blue\") = 0.50. When the color word simply deterministically \n",
    "# decides that a pin of the stated color is to be chosen, the pins of the stated color are the only, equally viable candidates.\n",
    "# S1 behaves analoguously. The size parameter is large enough to behave similarly as well.\n",
    "\n",
    "# The number of chains should not make a difference with fixed parameters.\n",
    "# For iter_warmup and iter_sampling, it is apparently best practice in a lot of cases to simply choose 500 because\n",
    "# models will converge between 200 and 400. Going above 200 made absolutely no difference for us though, using a forward model.\n",
    "fit = rsa_model.sample(stan_data, show_console=True, chains=1, iter_warmup=200, adapt_engaged=True, iter_sampling=200)\n",
    "# fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0:\n",
      "       big_blue  big_red  small_blue\n",
      "big        0.44     0.44        0.11\n",
      "small      0.17     0.17        0.67\n",
      "blue       0.50     0.01        0.50\n",
      "red        0.01     0.98        0.01\n",
      "S1:\n",
      "            big  small  blue  red\n",
      "big_blue   0.03   0.00  0.97 0.00\n",
      "big_red    0.00   0.00  0.00 1.00\n",
      "small_blue 0.00   1.00  0.00 0.00\n"
     ]
    }
   ],
   "source": [
    "# Visualize results for exercise 3.2.2. \n",
    "# Exact same code as we used for exercise 3.1\n",
    "\n",
    "L0_draws = fit.stan_variable('L0')\n",
    "S1_draws = fit.stan_variable('S1')\n",
    "\n",
    "L0 = L0_draws[0]\n",
    "S1 = S1_draws[0]\n",
    "\n",
    "df_L0 = pd.DataFrame(L0, index=utterances, columns=states)\n",
    "df_S1 = pd.DataFrame(S1, index=states, columns=utterances)\n",
    "\n",
    "print(\"L0:\")\n",
    "print(df_L0.to_string(float_format=\"{:.2f}\".format))\n",
    "print(\"S1:\")\n",
    "print(df_S1.to_string(float_format=\"{:.2f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6141ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.2.3\n",
    "# The heat map generated by this cell shows P(\"blue\"|\"big_blue\") for S1 given different size and color parameters.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This function was added (out of frustration) because the heat map just would not work at first.\n",
    "# It is (as a beginner) not always intuitive how indexing works in Stan.\n",
    "# The first versions of this cell generated heat maps that were only one color.\n",
    "# Printing out the matrix showed that we read the indices in a confused manner. \n",
    "# This is possibly bloated, but it makes the code below more readable. \n",
    "def extract_matrix(samples, prefix, n_row, n_col):\n",
    "    M = np.zeros((n_row, n_col))\n",
    "    for r in range(1, n_row + 1):\n",
    "        for c in range(1, n_col + 1):\n",
    "            col = f\"{prefix}[{r},{c}]\"\n",
    "            M[r - 1, c - 1] = samples[col].mean()\n",
    "    return M\n",
    "\n",
    "# We want to run the model for different values. This function will later be called from inside a loop to accomplish just that.\n",
    "def run_rsa_for(x_size, x_color, rsa_model, base_stan_data):\n",
    "    # The base data is copied to avoid potential side effects.\n",
    "    stan_data = base_stan_data.copy()\n",
    "    \n",
    "    # This is familiar from the code for exercise 3.2.2.\n",
    "    # The different values we want to compare will be plugged in here.\n",
    "    stan_data[\"x_size\"] = x_size\n",
    "    stan_data[\"x_color\"] = x_color\n",
    "\n",
    "    # Same setup as in exercise 3.2.2\n",
    "    fit = rsa_model.sample(\n",
    "        stan_data,\n",
    "        show_console=False,\n",
    "        chains=1,            \n",
    "        iter_warmup=200,\n",
    "        iter_sampling=200,\n",
    "        adapt_engaged=True\n",
    "    )\n",
    "    \n",
    "    # Extract transformed parameters\n",
    "    samples = fit.draws_pd()\n",
    "    \n",
    "    # Return mean values of L0 and S1.\n",
    "    # Printing these out we get \"L0[1,2]\", \"S1[2,3]\", and so on and so forth.\n",
    "    # This was helpful for troubleshooting, like the Stan manual suggests.\n",
    "    L0_cols = [c for c in samples.columns if c.startswith(\"L0[\")]\n",
    "    S1_cols = [c for c in samples.columns if c.startswith(\"S1[\")]\n",
    "\n",
    "    U = stan_data[\"U\"]\n",
    "    S = stan_data[\"S\"]\n",
    "\n",
    "    L0_mean = extract_matrix(samples, \"L0\", U, S)   # U × S\n",
    "    S1_mean = extract_matrix(samples, \"S1\", S, U)   # S × U\n",
    "\n",
    "    return L0_mean, S1_mean\n",
    "\n",
    "# We will try the values 0.1, 02..0.9, 1.0 for size and color.\n",
    "# This simply seemed like the obvious choice, and it results in an interesting visualization.\n",
    "x_size_vals = np.linspace(0.1, 1.0, 10)\n",
    "x_color_vals = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Here we store the results for visualization.\n",
    "results = {}\n",
    "\n",
    "# This is the loop announced above that runs the model for each value-pair.\n",
    "for xs in x_size_vals:\n",
    "    for xc in x_color_vals:\n",
    "\n",
    "        # Rebuild meaning matrix\n",
    "        # This is the same code as for exercise 3.2.2, except that it includes the variables xs and xc.\n",
    "        # Otherwise we would not actually iterate over different value-pairs.\n",
    "        meaning_matrix = np.zeros((n_utterances, n_states))\n",
    "        for u, utterance in enumerate(utterances):\n",
    "            for s, state in enumerate(states):\n",
    "                extension = utterance in state\n",
    "                if utterance in size_utterances:\n",
    "                    x = xs\n",
    "                elif utterance in color_utterances:\n",
    "                    x = xc\n",
    "                meaning_matrix[u, s] = x if extension else (1 - x)\n",
    "\n",
    "        # Update stan_data\n",
    "        stan_data[\"meaning_matrix\"] = meaning_matrix.tolist()\n",
    "        \n",
    "        # Run RSA\n",
    "        L0, S1 = run_rsa_for(xs, xc, rsa_model, stan_data)\n",
    "        results[(xs, xc)] = S1\n",
    "\n",
    "# We could plot any utterance.\n",
    "utterance_index = utterances.index(\"blue\")  \n",
    "# We could plot any state\n",
    "state_index = states.index(\"big_blue\")      \n",
    "\n",
    "# Standard heat map. \n",
    "# ChatGPT was used here via the following prompt: \"Give me Python code to generate a heat map for 10 value pairs\".\n",
    "# Inserting our own variables, labels, and description resulted in the code below, which worked without further modifications.\n",
    "# (As mentioned above, the heat map only contained one color at first, but that was because we had read in the matrix wrong.)\n",
    "heatmap = np.zeros((len(x_size_vals), len(x_color_vals)))\n",
    "\n",
    "for i, xs in enumerate(x_size_vals):\n",
    "    for j, xc in enumerate(x_color_vals):\n",
    "        heatmap[i, j] = results[(xs, xc)][state_index, utterance_index]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(heatmap, origin=\"lower\", \n",
    "           extent=[min(x_color_vals), max(x_color_vals), \n",
    "                   min(x_size_vals), max(x_size_vals)],\n",
    "           aspect=\"auto\")\n",
    "plt.colorbar(label=f\"P(Says '{utterances[utterance_index]}' | state='{states[state_index]}')\")\n",
    "plt.xlabel(\"x_color\")\n",
    "plt.ylabel(\"x_size\")\n",
    "plt.title(\"Speaker S1 probability heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab97ea9",
   "metadata": {},
   "source": [
    "# Exercise 3.2.3 Result Analysis and Interpretation\n",
    "\n",
    "The heat map shows what we expected it to. \n",
    "Let's look at the example P(\"blue\"|\"big_blue\") for S1. (Other utterances/states result in analoguous heat maps.*)\n",
    "The probability of saying \"blue\" increases with x_color. It decreases at the margins where x_size is very small or large.\n",
    "This is in line with what we discussed in 3.2.2. A more extreme color value places more importance on a color term, of course.\n",
    "If the color in \"big_blue\" is maximally important, S1 will always say \"blue\". When size is also (near) maximally important,\n",
    "P(\"blue\") is reduced because the size term might be used instead.\n",
    "\n",
    "*Now on to what we mean by \"analoguous\", in which we will describe the distribution for every pair and why it's as expected:\n",
    "\n",
    "Note that for every value-pair, entropy is maximal when they are both 0.5.\n",
    "\n",
    "For P(\"blue\"|\"big_blue\"), a sort of triangle shape emerges.\n",
    "P(blue|big_blue) increases with higher x_color and x_size closer to 0.5, so the central right edge of the map concentrates P.\n",
    "\n",
    "P(big|big_blue) basically rotates the shape by -90 degrees. P(big) increases with size and x_color closer to 0.5\n",
    "\n",
    "P(red|big_red) has a different shape. Because \"red\" only appears in this utterance, P(red) is maximal for x_color >= 0.8.\n",
    "When x_color = 0.6, P(red) is still maximal for x_size values close to 0.5. As with other value-pairs, more extreme size values\n",
    "shift the importance toward the size term, reducing P(red) toward the fringes. x_color values below 0.5 result in the size\n",
    "term being used instead.\n",
    "\n",
    "P(big|big_red) displays a far narrower version of the shape P(big|big_blue) displayed. Probability is maximal when \n",
    "x_size >= 0.7 and when x_color is close to 0.5. The same explanation as above holds: Extreme x_color values mean that the\n",
    "color term will be used instead of the size term. \"red\" only appears in one utterance, which obviously reduces its prior\n",
    "probability and narrows the range of values where it is likely to be chosen by S1.\n",
    "\n",
    "P(big|small_blue) has the shape of P(red|big_read) rotated by 90 degrees. It is maximal when x_size is small. When x_size\n",
    "is close to 0.5, x_color has to be as well. This is expected because \"big\" does not describe \"small_blue\" well, so little\n",
    "importance has to be placed on size in order for it to be selected. If size is closer to 0.5, extreme color values will lead\n",
    "to a color term being chosen instead.\n",
    "\n",
    "P(blue|small_blue) is the shape of P(big|big_red) rotated by 90 degrees: A narrow distribution with max. P when x_color is\n",
    "large and x_size is close to 0.5. Again, high importance placed on color will lead to \"blue\" being selected, but it would be\n",
    "rational to say \"small\" in this context, so even slightly more extreme x_size values will lead to \"smalL\" being said instead.\n",
    "\n",
    "P(blue|big_red) -> like P(big|small_blue) rotated by 90 degrees. \"blue\" does not occur in the state \"big_red\", so extremely\n",
    "low x_color values are the only viable way for it to be chosen.\n",
    "\n",
    "P(red|big_blue) -> probable when x_color is minimal (because of course red != blue) and when x_size is closer to 0.5 (because\n",
    "more extreme size values will favor using a size term instead) -> broad triangle shape as we've already seen above\n",
    "\n",
    "P(red|small_blue) -> narrow version of P(red|big_blue). Only viable when x_color is minimized and x_size is unimportant \n",
    "because it is the wrong color and any more extreme size value will favor a size term. \n",
    " \n",
    "=> To sum up, three shapes emerge on the heat maps in line with our expectations. \n",
    "Unfit utterances are only chosen when the value of their category is minimized.\n",
    "Fitting utterances are probable when their category value is large and the \"competing\" category value is not too extreme.\n",
    "-> For fitting utterances a triangle shape emerges. \n",
    "\n",
    "The triangle is narrow for P(big|big_red) and P(red|small_blue) because \"red\" only appears in one state, therefore the size\n",
    "of the competing category value does not need to be as high to reduce its probability. For P(blue|small_blue) a similar \n",
    "picture emerges because size is the category that immediately distinguishes the target from all other states, i.e. there\n",
    "is only one small blue pin.\n",
    "Utterances are probable when their category is given importance. Extreme values for competing categories reduce their \n",
    "probability. Narrow distributions reflect the fact that categories have different probabilities a priori. The distributions\n",
    "as seen on our heat maps thus successfully indicate the areas that rationally chosen utterances fall into."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fef420",
   "metadata": {},
   "source": [
    "# Exercise 3.2.4 Construct a context where color is sufficient to mention and interpret the output.\n",
    "\n",
    "(We basically did this in 3.2.3 by looking at every utterance/target pair.)\n",
    "\n",
    "So far we worked with the following states: [\"big_blue\", \"big_red\", \"small_blue\"]\n",
    "In the trial pictured (at this point far) above, the target is the small blue pin.\n",
    "We could simply modify the trial by defining the states as [\"big_red\", \"small_red\", \"small_blue\"] and still targeting\n",
    "\"small_blue\", where the color \"blue\" is as unambiguous as the size \"small\" is in the non-modified trial.\n",
    "\n",
    "This modification does not change any of the probability distributions. All the patterns described in the answer to 3.2.3\n",
    "reoccur. \n",
    "Uttering \"blue\" in this context is maximally probable when x_color is maximized and only influenced by x_size when the color\n",
    "values are closer to 0.5. This is because color is so informative. It is the same pattern as for, e.g., P(red|big_red) in 3.2.3.\n",
    "As in 3.2.3, P(small|small_blue) in the new context where color is sufficient results in a narrow triangular distribution\n",
    "because even slightly extreme x_color values beat size. Again because color is so informative here.\n",
    "Lastly, P(red|small_blue) inverts the shape of P(blue|small_blue). Because red is unfit, x_color has to be minimized for it\n",
    "to be selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f682f",
   "metadata": {},
   "source": [
    "### 3.3) Model evaluation by comparison to experiment data (42 points)\n",
    "3.3.1) Create a new file `sem_rsa.stan`. Adapt the vanilla RSA model in a way that allows you to infer all free parameters instead of specifying them beforehand. Condition the model on the observed production data (`data/data_exp1.csv`) and integrate over the free parameters. Preprocess the observed data in a way that you see fit for the modeling purpose. Assume uniform priors for each parameter. Use the generated quantities block in your Stan model to generate the posterior predictive distribution (read up [Stan documentation](https://mc-stan.org/docs/stan-users-guide/posterior-prediction.html) for this). Choose an appropriate number of iterations for warm up and sampling from the posterior. (16 points)\n",
    "\n",
    "    `stan/sem_rsa.stan` samples the pragmaticity parameter `alpha` as well as two cost parameters keyed to size and color indicators. The preprocessing cell builds the graded meaning matrix, utterance-specific indicator vectors, and per-trial state/utterance IDs from `data_exp1.csv`, so the Stan program can infer all three parameters jointly and generate posterior predictive utterances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 2076 trials spanning 1427 unique contexts.\n",
      "meaning_matrix shape: (3, 1427)\n",
      "Size indicator: [1, 0, 1]\n",
      "Color indicator: [0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess experiment data for the semantic RSA model\n",
    "# Load raw production data from the TSV file\n",
    "rsa_df = pd.read_csv('data_exp1.csv', sep='\t')\n",
    "\n",
    "# Map the three utterance classes onto integer IDs and drop the rest\n",
    "utterance_map = {'size': 1, 'color': 2, 'size and color': 3}\n",
    "rsa_df = rsa_df[rsa_df['UtteranceType'].isin(utterance_map)].reset_index(drop=True)\n",
    "rsa_df['utt_id'] = rsa_df['UtteranceType'].map(utterance_map).astype(int)\n",
    "\n",
    "# Build a unique state key so we can collapse identical contexts\n",
    "state_key_cols = [\n",
    "    'TargetItem',\n",
    "    'SufficientProperty',\n",
    "    'RedundantProperty',\n",
    "    'NumDistractors',\n",
    "    'NumSameDistractors',\n",
    "    'SceneVariation',\n",
    "]\n",
    "rsa_df['state_key'] = rsa_df[state_key_cols].astype(str).agg(' | '.join, axis=1)\n",
    "state_keys = rsa_df['state_key'].unique()\n",
    "state_lookup = {key: idx + 1 for idx, key in enumerate(state_keys)}\n",
    "rsa_df['state_id'] = rsa_df['state_key'].map(state_lookup).astype(int)\n",
    "\n",
    "U = len(utterance_map)\n",
    "S = len(state_lookup)\n",
    "N = len(rsa_df)\n",
    "\n",
    "# Keep the alphabetized target list for later grouping in the analysis\n",
    "target_names = sorted(rsa_df['TargetItem'].unique())\n",
    "\n",
    "# Precompute binary indicators for whether an utterance mentions size and/or color\n",
    "ordered_utts = sorted(utterance_map.items(), key=lambda kv: kv[1])\n",
    "category_lookup = {\n",
    "    'size': (1, 0),\n",
    "    'color': (0, 1),\n",
    "    'size and color': (1, 1),\n",
    "}\n",
    "size_indicator = np.zeros(U, dtype=int)\n",
    "color_indicator = np.zeros(U, dtype=int)\n",
    "for utt, idx in ordered_utts:\n",
    "    size_indicator[idx - 1], color_indicator[idx - 1] = category_lookup[utt]\n",
    "\n",
    "# Columns that list the distractor object names\n",
    "alt_cols = [f'alt{i}Name' for i in range(1, 5)]\n",
    "epsilon = 1e-3 # so that we don't get zeros\n",
    "\n",
    "\n",
    "# Parse strings such as 'small_blue_belt' into size/color attributes\n",
    "def parse_item(name):\n",
    "    if pd.isna(name) or name == 'undefined':\n",
    "        return None\n",
    "    parts = str(name).split('_')\n",
    "    if len(parts) < 3:\n",
    "        return None\n",
    "    return {'size': parts[0], 'color': parts[1]}\n",
    "\n",
    "\n",
    "# Gather the target plus distractors for each trial\n",
    "def collect_objects(row):\n",
    "    objs = []\n",
    "    target = parse_item(row['TargetItem'])\n",
    "    if target is None:\n",
    "        raise ValueError(f\"Missing target description for row {row.name}\")\n",
    "    objs.append(target)\n",
    "    for col in alt_cols:\n",
    "        item = parse_item(row.get(col))\n",
    "        if item is not None:\n",
    "            objs.append(item)\n",
    "    num_required = int(row['NumDistractors']) + 1\n",
    "    if len(objs) < num_required:\n",
    "        raise ValueError(f\"Context {row.name} expected {num_required} objects, got {len(objs)}\")\n",
    "    return objs[:num_required]\n",
    "\n",
    "\n",
    "# Compute graded semantics scores per context from the object lineup\n",
    "def semantics_from_context(row):\n",
    "    objs = collect_objects(row)\n",
    "    target = objs[0]\n",
    "    total = len(objs)\n",
    "    distractors = max(total - 1, 1)\n",
    "\n",
    "    size_matches = sum(1 for obj in objs if obj['size'] == target['size'])\n",
    "    color_matches = sum(1 for obj in objs if obj['color'] == target['color'])\n",
    "    both_matches = sum(\n",
    "        1\n",
    "        for obj in objs\n",
    "        if obj['size'] == target['size'] and obj['color'] == target['color']\n",
    "    )\n",
    "\n",
    "    size_conf = (size_matches - 1) / distractors\n",
    "    color_conf = (color_matches - 1) / distractors\n",
    "    both_conf = (both_matches - 1) / distractors\n",
    "\n",
    "    size_score = 1.0 - size_conf\n",
    "    color_score = 1.0 - color_conf\n",
    "    both_score = 1.0 - both_conf\n",
    "\n",
    "    if row['SufficientProperty'] == 'size':\n",
    "        size_score = min(1.0, size_score + 0.15)\n",
    "    else:\n",
    "        color_score = min(1.0, color_score + 0.15)\n",
    "\n",
    "    if 'size' in row['RedundantProperty']:\n",
    "        size_score = max(0.05, size_score - 0.1)\n",
    "    if 'color' in row['RedundantProperty']:\n",
    "        color_score = max(0.05, color_score - 0.1)\n",
    "\n",
    "    both_score = np.clip(both_score + max(size_conf, color_conf) * 0.5, 0.0, 1.0)\n",
    "\n",
    "    return (\n",
    "        # make sure there are no zeros \n",
    "        np.clip(size_score, 0.05, 1.0),\n",
    "        np.clip(color_score, 0.05, 1.0),\n",
    "        np.clip(both_score, 0.05, 1.0),\n",
    "    )\n",
    "\n",
    "\n",
    "# Build the meaning matrix once per unique state using the helper above\n",
    "meaning_matrix = np.zeros((U, S), dtype=float)\n",
    "state_reps = rsa_df.drop_duplicates('state_key', keep='first')\n",
    "\n",
    "for _, state_row in state_reps.iterrows():\n",
    "    sid = state_lookup[state_row['state_key']] - 1\n",
    "    size_score, color_score, both_score = semantics_from_context(state_row)\n",
    "    meaning_matrix[utterance_map['size'] - 1, sid] = epsilon + (1 - epsilon) * size_score\n",
    "    meaning_matrix[utterance_map['color'] - 1, sid] = epsilon + (1 - epsilon) * color_score\n",
    "    meaning_matrix[utterance_map['size and color'] - 1, sid] = epsilon + (1 - epsilon) * both_score\n",
    "\n",
    "# Sanity checks for Stan compatibility\n",
    "assert meaning_matrix.shape == (U, S)\n",
    "assert meaning_matrix.min() > 0.0\n",
    "\n",
    "# Hand everything to Stan, including the new indicator arrays\n",
    "stan_data_sem = {\n",
    "    'S': S,\n",
    "    'U': U,\n",
    "    'N': N,\n",
    "    'meaning_matrix': meaning_matrix.tolist(),\n",
    "    'size_indicator': size_indicator.astype(int).tolist(),\n",
    "    'color_indicator': color_indicator.astype(int).tolist(),\n",
    "    'state_id': rsa_df['state_id'].to_numpy(dtype=int).tolist(),\n",
    "    'utt_id': rsa_df['utt_id'].to_numpy(dtype=int).tolist(),\n",
    "}\n",
    "\n",
    "# Log output\n",
    "print(f'Prepared {N} trials spanning {S} unique contexts.')\n",
    "print('meaning_matrix shape:', meaning_matrix.shape)\n",
    "print('Size indicator:', size_indicator.tolist())\n",
    "print('Color indicator:', color_indicator.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "size_score, color_score, both_score = semantics_from_context(state_row)\n",
    "print(size_score)\n",
    "print(color_score)\n",
    "print(both_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbdef879",
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_rsa_file = os.path.join('stan', 'sem_rsa.stan')\n",
    "sem_rsa_model = CmdStanModel(stan_file=sem_rsa_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae6e6733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:04:16 - cmdstanpy - INFO - CmdStan start processing\n",
      "chain 1:   0%|\u001b[33m          \u001b[0m| 0/800 [00:00<?, ?it/s, (Warmup)]\n",
      "\u001b[A\n",
      "\n",
      "chain 1:  12%|\u001b[33m█▎        \u001b[0m| 100/800 [00:04<00:32, 21.49it/s, (Warmup)]\n",
      "\u001b[A\n",
      "\n",
      "chain 1:  38%|\u001b[33m███▊      \u001b[0m| 300/800 [00:07<00:10, 48.74it/s, (Warmup)]\n",
      "\u001b[A\n",
      "\n",
      "chain 1:  50%|\u001b[34m█████     \u001b[0m| 400/800 [00:08<00:07, 55.13it/s, (Sampling)]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "chain 1:  62%|\u001b[34m██████▎   \u001b[0m| 500/800 [00:10<00:05, 52.87it/s, (Sampling)]\n",
      "\u001b[A\n",
      "\n",
      "chain 1:  75%|\u001b[34m███████▌  \u001b[0m| 600/800 [00:13<00:04, 48.96it/s, (Sampling)]\n",
      "\u001b[A\n",
      "\n",
      "chain 1:  88%|\u001b[34m████████▊ \u001b[0m| 700/800 [00:15<00:02, 46.13it/s, (Sampling)]\n",
      "\u001b[A\n",
      "\n",
      "chain 1: 100%|\u001b[34m██████████\u001b[0m| 800/800 [00:18<00:00, 42.28it/s, (Sampling)]\n",
      "\u001b[A\n",
      "\n",
      "chain 1: 100%|\u001b[34m██████████\u001b[0m| 800/800 [00:19<00:00, 40.56it/s, (Sampling completed)]\n",
      "chain 2: 100%|\u001b[34m██████████\u001b[0m| 800/800 [00:19<00:00, 40.56it/s, (Sampling completed)]\n",
      "\n",
      "chain 3: 100%|\u001b[34m██████████\u001b[0m| 800/800 [00:19<00:00, 40.56it/s, (Sampling completed)]\n",
      "\n",
      "\n",
      "chain 4: 100%|\u001b[34m██████████\u001b[0m| 800/800 [00:19<00:00, 40.57it/s, (Sampling completed)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:04:36 - cmdstanpy - INFO - CmdStan done processing.\n",
      "21:04:36 - cmdstanpy - WARNING - Non-fatal error during sampling:\n",
      "Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in 'sem_rsa.stan', line 58, column 4 to column 46)\n",
      "Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in 'sem_rsa.stan', line 58, column 4 to column 46)\n",
      "Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in 'sem_rsa.stan', line 58, column 4 to column 46)\n",
      "Exception: categorical_lpmf: Probabilities parameter is not a valid simplex. sum(Probabilities parameter) = -nan, but should be 1 (in 'sem_rsa.stan', line 58, column 4 to column 46)\n",
      "Consider re-running with show_console=True if the above output is unclear!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sem_inits = [{'alpha': 2.0, 'cost_size': 0.5, 'cost_color': 0.5} for _ in range(4)]\n",
    "\n",
    "sem_fit = sem_rsa_model.sample(\n",
    "    data=stan_data_sem,\n",
    "    chains=4,\n",
    "    iter_warmup=400,\n",
    "    iter_sampling=400,\n",
    "    seed=123,\n",
    "    inits=sem_inits,\n",
    "    show_progress=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77f290ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking sampler transitions treedepth.\n",
      "Treedepth satisfactory for all transitions.\n",
      "\n",
      "Checking sampler transitions for divergences.\n",
      "No divergent transitions found.\n",
      "\n",
      "Checking E-BFMI - sampler transitions HMC potential energy.\n",
      "E-BFMI satisfactory.\n",
      "\n",
      "Rank-normalized split effective sample size satisfactory for all parameters.\n",
      "\n",
      "Rank-normalized split R-hat values satisfactory for all parameters.\n",
      "\n",
      "Processing complete, no problems detected.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>StdDev</th>\n",
       "      <th>R_hat</th>\n",
       "      <th>ESS_bulk</th>\n",
       "      <th>ESS_tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>2.828990</td>\n",
       "      <td>0.102537</td>\n",
       "      <td>1.00217</td>\n",
       "      <td>1437.52</td>\n",
       "      <td>1141.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cost_size</th>\n",
       "      <td>0.749837</td>\n",
       "      <td>0.068854</td>\n",
       "      <td>1.00142</td>\n",
       "      <td>1491.38</td>\n",
       "      <td>1318.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cost_color</th>\n",
       "      <td>0.005514</td>\n",
       "      <td>0.005763</td>\n",
       "      <td>1.00081</td>\n",
       "      <td>1234.92</td>\n",
       "      <td>737.077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Mean    StdDev    R_hat  ESS_bulk  ESS_tail\n",
       "alpha       2.828990  0.102537  1.00217   1437.52  1141.820\n",
       "cost_size   0.749837  0.068854  1.00142   1491.38  1318.370\n",
       "cost_color  0.005514  0.005763  1.00081   1234.92   737.077"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnostics = sem_fit.diagnose()\n",
    "print(diagnostics)\n",
    "\n",
    "sem_summary = sem_fit.summary()\n",
    "params_of_interest = sem_summary.loc[\n",
    "    ['alpha', 'cost_size', 'cost_color'],\n",
    "    ['Mean', 'StdDev', 'R_hat', 'ESS_bulk', 'ESS_tail'],\n",
    "]\n",
    "params_of_interest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d88136",
   "metadata": {},
   "source": [
    "3.3.2) Diagnose the model convergence and take actions if necessary. (4 points)\n",
    "\n",
    "    `diagnose()` reported no divergences, treedepth hits, or BFMI problems for the 4×(400 warmup + 400 sample) run. The largest R-hat across the three tuned parameters was ≈1.002 with bulk ESS well above 1k, so no further tuning was necessary despite the occasional simplex warning.\n",
    "\n",
    "3.3.3) Interpret a summary of the fitted model. (6 points)\n",
    "\n",
    "    Posterior means are `alpha ≈ 2.83 ± 0.10`, `cost_size ≈ 0.75 ± 0.07`, and `cost_color ≈ 0.006 ± 0.006`. Speakers therefore behave slightly pragmatically, strongly penalize size modifiers, and treat color mentions as essentially free. Redundant utterances inherit both penalties, but because color mentions barely change the cost for redundant utterances (compared to just size utterances), overinformative utterances will be favoured for informativeness reasons: When size should be mentioned, color can be addded \"for free\".\n",
    "    For comparison, the single-cost run produced `alpha ≈ 3.17` with `cost_weight ≈ 0.028`, which effectively removed any penalty for redundancy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9245adbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              target  empirical_overinform  posterior_mean_overinform\n",
      "0  big_black_avocado              0.200000                   0.176802\n",
      "1     big_black_belt              0.466667                   0.176300\n",
      "2     big_black_book              0.181818                   0.147508\n",
      "3     big_black_comb              0.076923                   0.152157\n",
      "4   big_black_turtle              0.187500                   0.161609\n",
      "Pearson r between empirical and posterior means: 0.262\n"
     ]
    }
   ],
   "source": [
    "S1_draws = sem_fit.stan_variable('S1')\n",
    "S1_mean = S1_draws.mean(axis=0)\n",
    "\n",
    "redundant_idx = utterance_map['size and color'] - 1\n",
    "state_indices = rsa_df['state_id'].to_numpy(dtype=int) - 1\n",
    "posterior_per_trial = S1_mean[state_indices, redundant_idx]\n",
    "\n",
    "posterior_overinform = (\n",
    "    rsa_df.assign(posterior_overinform=posterior_per_trial)\n",
    "          .groupby('TargetItem')['posterior_overinform']\n",
    "          .mean()\n",
    "          .reindex(target_names)\n",
    "          .to_numpy()\n",
    ")\n",
    "\n",
    "empirical_overinform = (\n",
    "    rsa_df.assign(overinform=rsa_df['UtteranceType'] == 'size and color')\n",
    "          .groupby('TargetItem')['overinform']\n",
    "          .mean()\n",
    "          .reindex(target_names)\n",
    "          .to_numpy()\n",
    ")\n",
    "\n",
    "corr = np.corrcoef(empirical_overinform, posterior_overinform)[0, 1]\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        'target': target_names,\n",
    "        'empirical_overinform': empirical_overinform,\n",
    "        'posterior_mean_overinform': posterior_overinform,\n",
    "    }\n",
    ")\n",
    "print(comparison_df.head())\n",
    "print(f'Pearson r between empirical and posterior means: {corr:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7058c",
   "metadata": {},
   "source": [
    "3.3.4) Correlate the model's posterior predictive distribution for overinformative utterance probabilities with the empirical data to assess and interpret model fit to the data. (8 points)\n",
    "\n",
    "    With the two cost parameters the Pearson r between empirical and posterior redundant rates climbs to ≈0.26 (up from ≈−0.30 under the single-cost model). The model now recovers some of the ordering across targets, but the signal is still weak compared with the empirical variability. The model’s predictions are more compressed and never reach the high empirical values for some objects. It captures part of the structure in the data, but it still underestimates the strength of the item-specific differences.\n",
    "\n",
    "3.3.5) Bonus: Introduce separate cost parameters for size and color. (6 bonus points)\n",
    "\n",
    "    The single cost parameter was replaced with two non-negative cost parameters, cost_size and cost_color, and a binary indicators was added, telling Stan whether each utterance mentions size, color, or both. The redundant utterance (“size and color”) pays both costs.\n",
    "    In the posterior, cost_size is clearly larger (around 0.75), while cost_color is very close to zero (around 0.006). This means the model learns that size words are expensive, but adding a color word is almost free. With only one cost parameter, these two effects were forced to be the same and got averaged into a small overall cost.\n",
    "\n",
    "3.3.6) Interpret and discuss your findings. (8 points)\n",
    "\n",
    "    The parameter alpha shows that speakers care about informativeness, but the key result is the strong asymmetry in the inferred costs. Size words carry most of the cost, while color words are treated as very cheap.\n",
    "    As a result, the model is happy to add a color modifier on top of a size description, because it increases or maintains informativeness at almost no extra cost. This helps the model better match the observed high rate of “overinformative” size-and-color utterances and improves the fit compared to the single-cost version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e0a1f",
   "metadata": {},
   "source": [
    "## X) Reflection (no points, but mandatory)\n",
    "\n",
    "Reflect on your group work. What went well? What did not go well? \n",
    "\n",
    "Please note down the group members' team roles anonymously and reflect on how you filled this role.\n",
    "\n",
    "- Our team made up of an Implementer, Plant, and Monitor-Evaluator proved an adequate meshup. The implementer indeed implemented a large proportion of the solutions, although everyone rose to the occasion. We had a fruitful vibe, so the plant prosociality paid off. The Monitor-Evaluator did evaluate too, but could have pulled their weight a little more. Our initial division of labor turned out to be sensible, but it took us some time to figure out an approach to the last exercise. A more extensive early group work phase could have made this part less chaotic and the deadline less stress-inducing. \n",
    "\n",
    "- Generally, we expect that the division of labor as well as the different stenghts of everyone will become more relevant when we have the the bigger final project to work on. This homework is a good testbed though.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bddbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e11b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
